<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on pinkhello</title><link>https://pinkhello.cc/posts/</link><description>Recent content in Posts on pinkhello</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>pinkhello</copyright><lastBuildDate>Thu, 08 Sep 2022 15:03:20 +0800</lastBuildDate><atom:link href="https://pinkhello.cc/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>分布式系统下的日志收集分析方案</title><link>https://pinkhello.cc/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90%E6%96%B9%E6%A1%88/</link><pubDate>Thu, 08 Sep 2022 15:03:20 +0800</pubDate><guid>https://pinkhello.cc/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E4%B8%8B%E7%9A%84%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%88%86%E6%9E%90%E6%96%B9%E6%A1%88/</guid><description>前言 分布式系统下，大规模集群应用分布在数台甚至成百上千台不同的服务器上，而日志又是我们开发人员用于定位问题、监控系统健康度、用于报警的最基础的数据来源。要想快速方便的对日志进行查找、分析归纳、归档、报警等功能，使用传统的 Linux 命令去查看日志，肯定不现实。所以一般的分布式集群系统环境下，我们都会去构建一个集中的日志系统，可以将各个服务日志收集起来。从而达到我们需要的的效果。
业务规模复杂导致构建分布式日志系统的难点:
导致的日志多节点、日志量也在爆炸式的增长; 日志格式的不规范与不确定性; 日志的形式的复杂化(物理机/虚拟机、K8S、Docker，Application 等等),容器的标准输出，容器内文件、容器事件、K8S事件等等; 环境复杂、在K8S环境下的，机器宕机、上下线、POD销毁、扩容缩容引起的日志的更改，都需要瞬时的收集起来; 日志的种类多（Nginx 、CDN、Ingress、ServiceMesh Pod等等日志，审计日志、应用日志、中间件日志、调用链日志、网络日志等等） &amp;hellip;&amp;hellip; 日志的特点:
大量的数据, 流式数据； 上述的步骤，应该要解耦 要具体扩展性，数据量增加的时候，支持水平扩展。 开源的日志系统 我们要构建一个企业级的日志系统的，一般选择的有免费的和收费的，收费的，云厂商的日志服务，我们今天不在考虑范围内。免费的开源的主要有：
ELK stack GrayLog Loki 日志的产生到使用，主要经过下面的阶段： 采集 -&amp;gt; 传输 -&amp;gt; 缓冲 -&amp;gt; 处理 -&amp;gt; 存储 -&amp;gt; 检索
ELK stack ELK stack 是由 Elasticsearch、Logstash、Kibana 开源组件集合分析、搜索、展示的日志系统解决方案。
Elatsicsearch 是全文搜索分析引擎 Logstash 日志聚合器，收集和处理多种数据源的数据、转换并发送到其他目的地 Kibana 用户界面，用于可视化的查询分析数据 Beats 用于数据收集数据 GrayLog GrayLog 也是一款开源的日志解决方案，本身 Graylog 提供了日志收集、日志存储搭配 MongoDB，查询层面依赖 Elasticsearch, Github 地址 github
Graylog 提供对外接口，Web页面 Elasticsearcg 日志文件的持久化存储和检索 MongoDB 存储 Graylog 的配置 Graylog 一体化方案、方便、但是没有分层。因为不是 agent 方式，其支持的日志收集方式有 RestApi、UDP、TCP。 整体来说，通过构建 Input 来收集日志，每个 Input 可以单独配置 Extractors 来做字段转换。 当然还有其他的概念，诸如</description></item><item><title>重学Redis</title><link>https://pinkhello.cc/posts/60-redis/</link><pubDate>Mon, 29 Aug 2022 15:15:21 +0800</pubDate><guid>https://pinkhello.cc/posts/60-redis/</guid><description>redis redis 是BSD许可的开源内存数据中间件，可以用作 数据库、缓存、消息Broker、流引擎。
redis 数据类型 data structures:
strings hashes lists sets sorted sets [range queries] bitmaps hyperloglogs geospatial indexes streams redis key 的推荐规则 keys rule:
不要设置过长的KEY 过短的KEY也不建议 适中，并且能达意的，考虑用分隔符 最大的KEY SIZE: 512MB. String Redis 存储字符序列、可以是 文本、序列话对象、二进制数组。支持 incr 操作。
SET SETNX GET MEGT 大部分字符串操作都是 O(1), 但是 SUBSTR、GETRANGE、SETRANGE 可能是 O(n)
Redis String 是才赢得 SDS 实现的（Simple dynamic string）
在执行 set hello helloval后底层是
那么键的字符串对象，底层创建保存 &amp;lsquo;hello&amp;rsquo; 的SDS 那么值也是字符串对象，底层创建保存字符串的 &amp;lsquo;helloval&amp;rsquo; 的SDS SDS 的结构体
struct sdshdr { char buf[]; //字节数组，用于保存字符串 int len; //记录buf数组中已经使用字节的数量 int free; //记录buf数组内未使用的字节的数量 } 对比C，有专门的字段记录buf数组的字符串长度和空闲 时间复杂度O(1) 对比C，记录字符串了长度，在进行分配的时候，或修改的时候，杜绝了缓冲区溢出的可能性，在SDS API对SDS进行修改的时候，会先检查空间是否满足修改的需求，不满足的话，会先拓展至执行所需要的大小，再执行修改操作。 减少字符串修改带来的内存重分配操作， 通过 free 参数记录未使用的空间，这样实现了空间预分配和惰性空间释放策略 二进制安全。因为有 len 属性，不需要判断字符串确认是否是末尾。可以存储任意结构的二进制数据 兼容部分C函数 Lists 通过链表实现的、即使有数百万元素，在列表头部或尾部添加新元素的操作都是O(1): LPUSH 同时向10个元素和100万个元素添加头部元素的速度是相同的。</description></item><item><title>关于团队的思考</title><link>https://pinkhello.cc/posts/59-%E5%85%B3%E4%BA%8E%E5%9B%A2%E9%98%9F%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Sat, 20 Aug 2022 14:38:10 +0800</pubDate><guid>https://pinkhello.cc/posts/59-%E5%85%B3%E4%BA%8E%E5%9B%A2%E9%98%9F%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>团队本质 团队或公司是一个商业性的团体 诉求: 用户的增长、持续的盈利 以技术或产品角度思考：产品或服务 -&amp;gt; 获客 -&amp;gt; 盈利。 团队特性 团队是一直在变化的，一个稳定的组织要么垄断，要么死亡 团队下所有人会分化成不同的类型, 不是所有人都能跟上组织的发展、也不是所有人都有相同的工作能力或管理能力 不要过于在意你在团队中的角色、要多去想你要做到什么程度，为此你要付出什么样的努力或代价 团队的向上通道要永远打开、不能让其变成一潭死水 尊重团队每个人的意愿，有的人专注技术对管理毫无兴趣、有人喜欢总览进度而且还能让所有人都服气&amp;hellip; 选择有职业操守与共同利益的人成为你的团队成员 团队协同 自由意味着责任 明确团队成员 产出 和 Deadline、不做任何限制【甚至可以穿着拖鞋、睡衣工作，在咖啡馆提交代码】 不强制打卡工具限制工作、或者在办公电脑上装监控软件 明确必要的红线、诚信与道德方面，明确遵守契约精神、对于违反了准则、立即执行相应惩罚 尊重成员隐私、个人偏好、投资偏好、生活习惯（他是成年人、我们也不是家长） 鼓励团队突破上下游、尽量针对性的将一件完整的事交给匹配的成员，让其发现工作中不同的事带来的价值 所有的反馈在团队层面都是善意的 团队文化 开放、务实 让工作有价值，让工作本身带来价值感 组建团队博客、开放展览团队工作效果 不要吝啬赞扬、LGTM 是一个非常好的文化 Code Smell 与 Good Code，并组织 Hackathon 或参与外部 Hackathon 对事不对人、给予容错空间与包容 其他 会议 会议, { 没人喜欢开会 } 更准确的说 &amp;ldquo;{ 没人喜欢开低效率的会议 }&amp;rdquo; 限制会议人数, 人数越少, 工作连接性更好 同一个issue、同一个工作相关的才开、无关人不需要参与 会议控制 15分钟以内，不要超过 30分钟，控制每个人的发言时间。不要把会议演变成一个讨论 会议明确组织者、会议Owner需要明确 定期会议设置成自动化，是工作中的一部分 绩效 绩效的定义的初衷应该是 团队的动态的自我纠错，借用技术术语是 动态平衡容错。 绩效是对每个人相对合理的待遇和评价 绩效表现评定 360 度、玩真的 不要制造恐惧、评级达不到要求，直接告诉具体原因，并给出建议、留出充分的时间改进 当面讨论、不要背后非议、坦诚指出问题就是给出进步的机会 上级要充分的告知组员、预期是什么、什么样为合格、做到什么是棒 共事人 在技术峰会、展开遇到的问题与讨论，展示自己另一面、兴趣爱好、投资计划、旅行见闻 不要用爱发电，只能用爱维护团队关系 选择精英、杜绝内耗和写BUG 工具 应用提升自动化工具、减少或杜绝重复性工作 信息安全罗列入团队最高等级、也是红线等级 法律 必要的法律知识、明确所有事情在法律允许范围【删库跑路、薅羊毛入刑】 非开源、信息安全、职业操守的保持 OKR 团队KR = 个人的 O OKR 的 KRs 应该是可衡量的。且 OKR 是挑战性的但不要让人绝望。 对OKR、应该抱有 50% 的信心</description></item><item><title>记一次Airflow任务丢失问题</title><link>https://pinkhello.cc/posts/57-%E8%AE%B0%E4%B8%80%E6%AC%A1airflow%E4%BB%BB%E5%8A%A1%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 13 Jun 2022 22:34:44 +0800</pubDate><guid>https://pinkhello.cc/posts/57-%E8%AE%B0%E4%B8%80%E6%AC%A1airflow%E4%BB%BB%E5%8A%A1%E4%B8%A2%E5%A4%B1%E9%97%AE%E9%A2%98/</guid><description>描述问题 前置条件:
Airflow 调度 版本 1.10.4 【比较老的版本】 Airflow Scheduler ，Airflow Worker ，通信使用的 Celery 模式是 RabbitMQ 模式 EMR 环境 问题现象:
Airflow WebServer 展现, 可以看出 4 个任务 一直处于 queued 状态，不进入 running 状态 Airflow Scheduler 日志记录 [2022-06-13 22:20:09,899] {scheduler_job.py:934} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 4 task instances ready to be queued [2022-06-13 22:20:09,899] {scheduler_job.py:961} INFO - DAG {{ DAG_ID }} has 0/10 running and queued tasks [2022-06-13 22:20:09,899] {scheduler_job.</description></item><item><title>Systemd服务笔记</title><link>https://pinkhello.cc/posts/56-systemd%E6%9C%8D%E5%8A%A1%E7%AC%94%E8%AE%B0/</link><pubDate>Mon, 09 May 2022 22:56:19 +0800</pubDate><guid>https://pinkhello.cc/posts/56-systemd%E6%9C%8D%E5%8A%A1%E7%AC%94%E8%AE%B0/</guid><description>前言 今天离线数仓调度 airflow 任务出现问题，经过查看服务状态，确定是 airflow worker 的存在问题，并行dag最大是数量16，但是发现不止16个, 而查看配置文件是对的，所以就怀疑是启动脚本存在问题。发现启动是以 systemd 服务化启动的，确定了是停止脚本的问题。所以回头来记录一下 systemd 的知识。
systemd 介绍 systemd 是 Linux 的系统基础组件的几个，提供了一个系统和服务管理，运行以 PID 为 1 并负责启动其他程序
systemd 特性 并行启动 按需启动Daemon进程 利用 cgroups 监视进程 支持快照和系统恢复 维护挂载点和自动挂载点 各个服务间的依赖关系精密控制 systemd 基本使用 分析场景
显示系统状态 systemctl status tree 状显示
激活的Unit systemctl 或 systemctl list-units 活动状态的服务
失败的Unit systemctl --failed 所有可用的Unit systemctl list-unit-files /usr/lib/systemd/system/ 和 /etc/systemd/system
显示指的Unit状态 systemctl status &amp;lt;PID&amp;gt; 具体一饿指的的信息
Unit使用
一个配置文件可以描述内容 .service 系统服务、 .mount 挂载点、 .sockets Socket .device 系统设备、.swap 交换分区 、.</description></item><item><title>JVM内存结构JMM</title><link>https://pinkhello.cc/posts/55-jvm%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84jmm/</link><pubDate>Wed, 04 May 2022 01:17:28 +0800</pubDate><guid>https://pinkhello.cc/posts/55-jvm%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84jmm/</guid><description>JVM 内存模型 从JVM的官方文档上可以看见，JVM的内存结构主要包含三块: 堆内存、方法区、栈
堆中又分 新生代 和 老年代, 新生代包含 Eden空间、From Survivor空间、To Survivor空间，也是线程共享的区域
方法区（非堆区）存储的 类信息、常量、静态变量等数据，是线程共享的区域，
栈 分为 本地方法栈 和 JVM虚拟机栈，还有个程序计数器 这三个都是线程私有的
Java Heap Java Heap 是 JVM 虚拟机管理的内存最大的一块，Java Heap 被所有的线程共享的内存区域，几乎所有的对象都在这里分配，也是 GC 管理的主要区域 Java Heap 是在物理上不连续的内存空间，只要逻辑上连续即可。
Method Area Method Area（非堆） 和 Java Heap 一样，是🧍各个线程共享的内存区域，用于存储已经被虚拟机加载的 类信息、常量、静态变量、即使编译器编译的代码等等
Program Counter Register Program Counter Register 程序计数器，较小的内存空间，是当前线程锁执行的字节码的行号指示器（唯一一个没有OOM的区域）
JVM Stacks JVM Stacks 也是线程私有的，它的生命周期和线程相同，虚拟机栈描述的 Java 方法执行的内存模型： 创建栈帧 Stack frame 用于存储 局部变量表、操作栈、动态链接、方法出口 等信息 每次方法被调用直至完成，都对应着一个栈帧在虚拟机栈中的入栈和出栈过程。
JVM 两种异常：
线程请求栈深度大于虚拟机允许的深度 StackOverflowError
JVM栈一般动态扩展的，如果无法申请足够的内存，会抛 OutOfMemoryError</description></item><item><title>JVM类加载机制</title><link>https://pinkhello.cc/posts/52-jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</link><pubDate>Tue, 03 May 2022 17:30:49 +0800</pubDate><guid>https://pinkhello.cc/posts/52-jvm%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6/</guid><description>类加载系统 JVM核心的结构里面有一个就是 类加载系统。 一个java代码经过编译器后变成class字节码文件，后续在运行时，字节码需要通过JVM的类加载系统执行运行。
类加载系统
类的加载过程 类的生命周期 类加载器种类 类加载机制 类生命周期 类的生命周期包括: 加载、链接（验证、准备、解析）、初始化、使用和卸载，
加载
通过类全限定名来获取二进制字节流、 字节流描述的静态存储结构转换为方法区的运行时数据结构 在Heap中生成一个代表这个类的Class对象，作为方法区访问这些数据的入口 链接
验证 确保被加载的类的正确性
文件格式验证，字节流是否符合Class文件格式，如0xCAFFBABE开头、版本号、常量类型等等 元数据验证, 多字节码描述性信息进行语义分析、确保符合规范 字节码验证 符号引用验证 准备 为类的 静态变量 分配内存，并将其初始化成默认值
解析 把类中的符号引用转换为直接引用(直接指向目标的指针、相对偏移量或间接定位到目标的句柄), 主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄、调用点限定符等符号引用进行。
类加载过程 加载、链接（验证、准备、解析）、初始化 属于 类加载的过程
类加载过程从上图可看见
第一个阶段 Loading 通过类的全限定名获取到该类的class二进制字节码流数据，将二进制流数据代表的静态存储结构转换成方法区运行时的数据结构
第二个阶段 Linking 验证 Verify: 保证class文件字节流是符合JVM的要求，确保安全 准备 Prepare: 为静态字段分配内存并设置初始化默认值（被final修饰的static字段不会设置，在编译阶段就分配了） 解析 Resolve: 解析是为了将常量迟的符号引用转换为直接引用(实际引用)，如果符号引用指向未被加载的类，或未被加载的类的字段或方法，那么解析触发了这个类的加载（但是不一定触发这个类的Linking和Initialization） 第三个阶段 Initialization 初始化阶段是执行类的构造方法，init() 的过程,(编译器自动收集类的所有变量赋值动作和静态代码块语句合并的） 若该类有父类,JVM保证父类先执行init然后执行子类的init
类加载器的分类 上面描述了类加载的机制，那么用于类加载的组件我们叫类加载器，而且类加载器是有分类的
Bootstrap ClassLoader 启动类加载器，是JVM内部实现的，Java语言程序无法直接操作，主要用来加载Java核心类库，诸如rt.jar、resources.jar、sun.boot.class.path目录下的包，用于JVM运行所需的包（一般只加载 包名为java、javax、sun开头的类）
它加载Extension ClassLoader和Application ClassLoader，并成为它们的父类加载器
Extension ClassLoader 扩展类加载器(sun.misc.Launcher$ExtClassLoader实现), 派生继承 java.lang.ClassLoader, 父类加载器是 启动类加载器, 加载 lib/ext 目录下的加载类库。可以将自己的包放到该目录下，就会自动加载进入</description></item><item><title>Java内存泄漏的方式</title><link>https://pinkhello.cc/posts/53-java%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%96%B9%E5%BC%8F/</link><pubDate>Tue, 03 May 2022 01:10:31 +0800</pubDate><guid>https://pinkhello.cc/posts/53-java%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E7%9A%84%E6%96%B9%E5%BC%8F/</guid><description>Java 会有内存泄漏么？ 我们都知道 Java 是自动管理内存的，且核心的就是GC机制，即然提出这个问题，那就是肯定是存在内存泄漏的问题的。只不过是以哪种方式而已
♻️ 判断一个对象可以被回收 我们都说内存被占用，是Java的对象的占用的，那么JVM判断一个对象是否可以被回收，一般两种方式:
引用计数： 每一个对象都有引用计数的属性，新增一个引用计数+1，引用释放计数-1，计数为0可以回收 可达性分析：从GC Roots开始搜索，搜索引用链路是否能到达GC Roots，如果没有任何引用链路相连时候，该对象就可不可用，JVM就判断为可回收对象, 这种可以解决循环引用问题 GC Roots对象有哪些:
虚拟机栈中引用的对象 方法区中静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI引用的对象 因为，引用计数的方法简单，但是存在一个致命的缺陷，假设对象A引用对象B，对象B又引用对象A，双方的引用计数都为1，假如程序上确实没有使用这两个对象慢但是JVM却无法对他们进行回收。也就是对象的循环引用问题。 但是可达性分析的方式，从GC ROOTS开始搜索，找寻是否可达对象，不可达的进行回收，解决了循环引用的问题。
所以市面上大部分主流的虚拟机都使用可达性分析 来判定对象是否被GC回收
JAVA 体系 内存问题有几种 内存溢出 OOM (out of memory), 申请分配内存时候，没有足够的可用空间出现OOM 内存泄漏 (memory leak), 已经申请分配的内存, 无法释放已经申请老的内存空间, 且一直在申请分配，但一直存在内存泄漏，内存泄漏堆积后，导致的无内存可以。 内存泄漏会导致OOM，但是OOM不完全因为内存泄漏。也可能是太多的大对象导致的。
如何分析内存泄漏问题 我们都知道内存泄漏是因为对象太多或者因为内存申请了但是无法释放导致的，所以针对内存泄漏的分析，一般也是围绕内存的对象的申请、释放等动作，对内存管理的信息进行统计、分析、可视化操作，根据这些信息进行判断释放有内存泄漏问题
工具主要有
JvisualVM JProfile Arthas Xpocket &amp;hellip;</description></item><item><title>rand库锁竞争优化</title><link>https://pinkhello.cc/posts/50-go-rand/</link><pubDate>Wed, 30 Mar 2022 00:07:36 +0800</pubDate><guid>https://pinkhello.cc/posts/50-go-rand/</guid><description>前言 今天，在写随机数的生成的时候，好奇 rand 库在 golang 的实现，就翻进去看一眼的。rand.go#293 代码
var globalRand = New(&amp;amp;lockedSource{src: NewSource(1).(*rngSource)}) 竟然是全局共享的一个 全局的 globalRand 的 对象。可以猜想到在多 goroutine 下，性能应该比较差，存在竞争的情况。
验证 随机验证一下, 验证一下
第一种情况，就是普通的并发情况下使用默认的 rand 库。使其共用一个 globalRand 对象。 func BenchmarkGlobalRand_test(b *testing.B) { b.RunParallel(func(p *testing.PB) { for p.Next() { rand.Intn(200) } }) } 输出
BenchmarkGlobalRand_test BenchmarkGlobalRand_test-8 16704190 72.53 ns/op 0 B/op 0 allocs/op BenchmarkGlobalRand_test-8 17005326 68.90 ns/op 0 B/op 0 allocs/op BenchmarkGlobalRand_test-8 17651659 66.54 ns/op 0 B/op 0 allocs/op BenchmarkGlobalRand_test-8 17879901 68.07 ns/op 0 B/op 0 allocs/op 第二种情况，在每个 goroutine 内创建一个 rand 对象，达到不共享的效果 func BenchmarkCustomRand_test(b *testing.</description></item><item><title>Gitlab CI CD</title><link>https://pinkhello.cc/posts/49-cicd/</link><pubDate>Mon, 28 Mar 2022 10:23:58 +0800</pubDate><guid>https://pinkhello.cc/posts/49-cicd/</guid><description>基本概念 CI，Continuous Integration 持续集成， 即在代码构建过程中持续的对代码进行集成、构建、自动化测试。 CD，Continuous Deployment 持续交付，即在代码构建完毕后，可以方便的进行部署上线，快速迭代交付产品。 Gitlab CI/CD 先看一张图，介绍了 Gitlab CI/CD 的工作流程
上图是一个通用的开发流程，从代码实现开始、提交，通过代码的改变触发 CI/CD pipeline，后续通过 CR 和 Approve，代码才合并进入分支/
更丰富的操作:
CI/CD pipelines pipelines 包含两个核心的，它是 CI/CD 顶级组件。
Jobs 定义了要做什么。(eg：compile job or test job) Stages 定义了如何做。 如果一个 Stage 中的所有的 Job 都成功，则 pipeline 进入下一个 Stage。 如果一个 Stage 中任何 Job 失败，一般不会到下一个 Stage，并且 pipeline 会提前结束。
典型的一个 pipeline 的操作:
build stage， 使用一个 job 去 compile 代码 test stage，使用 多个 job 去跑 ut test staing stage，使用 分布部署阶段 production stage，启动 job 生成部署阶段 Jobs pipeline 的配置从 job 开始了，job 是 .</description></item><item><title>Golang for Range 探究</title><link>https://pinkhello.cc/posts/51-golang-for-range-%E6%8E%A2%E7%A9%B6/</link><pubDate>Sun, 27 Mar 2022 14:20:28 +0800</pubDate><guid>https://pinkhello.cc/posts/51-golang-for-range-%E6%8E%A2%E7%A9%B6/</guid><description>前言 今天在用 Golang 写 TCP 长链接的时候，在做测试的时候 for range 有在结构包的时候有性能问题。所以就细化探究了一下。
复现 假设有一个 Packet 类型([1024]byte)的数据包,我们从网络中获取到一个 [1024]Packet 的数据包，我们进行遍历
type Packet [1024]byte func BenchmarkForStruct_test(b *testing.B) { var items [1024]Packet var result Packet for i := 0; i &amp;lt; b.N; i++ { for k := 0; k &amp;lt; len(items); k++ { result = items[k] } } _ = result } func BenchmarkRangeStruct_test(b *testing.B) { var items [1024]Packet var result Packet for i := 0; i &amp;lt; b.</description></item><item><title>细谈线程池设计与监控</title><link>https://pinkhello.cc/posts/47-%E7%BB%86%E8%B0%88%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1%E4%B8%8E%E7%9B%91%E6%8E%A7/</link><pubDate>Sat, 05 Mar 2022 22:05:23 +0800</pubDate><guid>https://pinkhello.cc/posts/47-%E7%BB%86%E8%B0%88%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1%E4%B8%8E%E7%9B%91%E6%8E%A7/</guid><description>前言 万年八股文，在工作中肯定多多少少都使用了线程池，但有没有真正的去了解线程池的整个运行机制和核心关键点。今天分享一下。
线程池简介 先说 线程(Thread) , 在 Wiki 的解释是操作系统能进行运算调度的最小单元。包含在进程中，是进程实际运作的单元。 OK，先记住这一点，我们说 线程池(Thread Pool) ，顾名思义，基于池化思想的管理线程的手段（工具）。既然叫线程池了，可以 知道肯定是多线程的服务环境下。
我们都知道Server的资源是有限的，不可能无止境的扩张（从成本、运维上考虑）。线程过分的开辟的话，会导致整体服务器资源吃紧（线程带来的开销、创建、调度等等）。 池化的思想，线程池维护着多个线程，等待调度分配并发任务去执行。一方面避免有任务就创建线程带来的额外的开销，另一方面也是保护服务器产生过多的线程数量。
线程池解决什么问题 线程池解决的核心问题是资源管理问题，在并发环境下，系统和服务并不知道在什么时候需要多少资源。而 池化(Pooling) 思想，就是为了将资源统一管理的情况下，减小风险的思想。
Wiki上 ThreadPool图
P.S : 池化思想举例
内存池(Memory Pool) 对象池(Object Pool) 即实例池 连接池(Connection Pool) 线程池核心设计 在看线程池的设计之前，先了解一下线程池的构造参数
corePoolSize – the number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set maximumPoolSize – the maximum number of threads to allow in the pool keepAliveTime – when the number of threads is greater than the core, this is the maximum time that excess idle threads will wait for new tasks before terminating.</description></item><item><title>LRUCache 的实现</title><link>https://pinkhello.cc/posts/46-lrucache-%E5%AE%9E%E7%8E%B0/</link><pubDate>Thu, 10 Feb 2022 18:59:50 +0800</pubDate><guid>https://pinkhello.cc/posts/46-lrucache-%E5%AE%9E%E7%8E%B0/</guid><description>文引 要实现 LRU Cache，我们需要了解 LRU Cache 的运行原理。
LRU Cache LRU (Least Recently Used) Cache 是一种缓存淘汰算法，最少最近使用的Cache。也就是说，在淘汰的时候，淘汰的是最长时间未使用（或者使用最少）的数据
如下图，容量是为 3 的缓存:
一开始，Cache 为空,添加一个 12 元素进入，后面依次放入了 11 和 10。这时候将缓存填满了。 将放入 9 的时候，因为缓存满了，需要移除最长时间未使用的数据。所以 12 被移除，9 被加入。 P.S.: 这边未考虑使用操作，其实也可以想像的到，如果要是使用了，就将使用的元素从缓存移除，重新加入到缓存中
经过上述了解，已经直到了 LRU 的运行机制，这样我们总结特点：
Cache 大小固定有限 Cache Full 后，后续操作都需要执行 LRU 操作 Cache 操作支持并发 Cache 的 添加、排序、获取 尽量都是 O(1) 操作 Structure of LRU Cache 要考虑设计 LRU Cache，我们需要根据其特点来设计：
特性1: LRU Cache 是一个 Queue，如果 Queue 里面的某个元素被访问了，它应该被迁移到淘汰的末位。 特性2:LRU Cache 要有固定容量（内存有限）, 当新加一个元素的时候，是添加到 Queue Head；当发送淘汰的时候，淘汰的是 Queue Tail。（FIFO） 特性3: 查找命中缓存的数据，必须在 最少的固定的时间内 完成。也就是我们我们必须尽量保证时间复杂度 O(1) 特性4: 移除最近最少使用的元素，也必须在 最少的固定的时间内 完成。同上 O(1) 要满足 特性1 和 特性2 还是很简单的,要实现一个 Queue 我们可以使用数据结构 固定大小的Linked List 或 固定大小的Array List 做实现。 但是在兼顾 特性3 的时候，可以想象到要尽量 添加、命中 都是 O(1) 操作，在 Queue 中基本不可能。但是我们可以预判到 HashMap 可以最大化的解决这个问题， 因为HashMap最理想的情况下达到 命中缓存为 O(1) 操作，到此，我们发现 LRU Cache 的设计应该是 HashMap + List； 下面再去考虑 特性4，在 Cache 满的情况下，考虑每次添加新的元素都要执行LRU策略，这时候可预见的对于这个 List的插入``删除比查询多， 肯定选择是 Linked List，且是 DoublyLinkedList 不是 SingleLinkedList (出于操作O(1)考虑)</description></item><item><title>如何去实现RingBuffer</title><link>https://pinkhello.cc/posts/45-%E5%A6%82%E4%BD%95%E5%8E%BB%E5%AE%9E%E7%8E%B0ringbuffer/</link><pubDate>Thu, 10 Feb 2022 14:39:59 +0800</pubDate><guid>https://pinkhello.cc/posts/45-%E5%A6%82%E4%BD%95%E5%8E%BB%E5%AE%9E%E7%8E%B0ringbuffer/</guid><description>文引 RingBuffer, 名如其意: 环形缓存区/环形队列，不同于一般的队列，特征是首尾相接。
RingBuffer 如何工作的 RingBuffer 是一个有界的循环的数据结构，主要用于多线程下进行的数据缓存。在持续的写入数据的时候，到达末尾的时候链接到头，效果上达成一个环状。
实现它的方式 它是有界的数组实现，如图。
而且我们还要关注到 reader指针 、 writer指针、 头尾相连
reader pointer 下一个可读元素 writer pointer 下一个可插入的元素 slot 数组头 和 数组尾 相互链接 讨论 ringbuffer 的运行方式 ringbuffer 的关键参数, read seq &amp;amp; write seq
reader pointer seq =&amp;gt; 从 0 开始，随着读取消耗一个元素 +1 writer pointer seq =&amp;gt; 从 -1 开始，插入一个元素时候 +1 可以看出两种 Seq 对 容量进行 Mod 操作可以将 seq 映射到 ringbuffer 的 index 上.
array_index = seq % capacity 基于上面的思想，我们看 ringbuffer 的核心操作</description></item><item><title>Golang的池化设计</title><link>https://pinkhello.cc/posts/43-golang%E7%9A%84%E6%B1%A0%E5%8C%96%E8%AE%BE%E8%AE%A1/</link><pubDate>Wed, 02 Feb 2022 02:52:28 +0800</pubDate><guid>https://pinkhello.cc/posts/43-golang%E7%9A%84%E6%B1%A0%E5%8C%96%E8%AE%BE%E8%AE%A1/</guid><description>Why Pool？ 先埋坑: Go 有没有池化的必要? 为什么要池化? Go 自从出生以来都被我们冠以&amp;quot;高并发&amp;quot;的Tag, 回头细想和深究一下，你会发现其实都是由 goroutine 实现的 , 我们都知道 与 Thread 相比， 创建 Goroutine 的代价非常小。从程序表现上，也像极了Thread, 每个 Program 至少包含一个 Goroutine(至少一个**Main Goroutine**) 所有的其他 Goroutines 都依附于 Main Goroutine（如果 Main Goroutine Terminated, 其他 Goroutines 也会 Terminated）, Goroutine 总是工作在后台。
多线程``多进程是为了提高系统的并发能力，当前系统下，这边就需要系统调度，一个线程可以拆分为 &amp;quot;用户态&amp;quot;线程 和 &amp;quot;内核态&amp;quot;线程，这两者需要进行绑定，我们一般称 内核态线程 为线程，用户态线程为协程
从上面可以看出 协程 和 线程 有映射关系，这样就来了 M:N 关系(为什么不是 1:1, N:1 ? )
Goroutine 优点:
相比 Thread 代价更小 Goroutine 其在 stack 的大小可以根据程序要求进行变化，但是 Thread 是固定的 Goroutine 使用 Channel 进行通信，Channel 用于 Goroutines 访问共享内存的时候防止竞争的。 从程序上一个线程可能拥有许多的 Goroutines 关联，如果这些关联的 有一个 Goroutine 去阻塞了线程，那么剩余的 Goroutine 将分配给其他的 OS Thread，且这种切换操作是对开发者屏蔽的。 Goroutine 调度器 回头看将 Goroutine 和 协程 有区别的。调度</description></item><item><title>MVCC在不同的数据库之中的应用</title><link>https://pinkhello.cc/posts/40-mvcc%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link><pubDate>Thu, 02 Dec 2021 11:06:28 +0800</pubDate><guid>https://pinkhello.cc/posts/40-mvcc%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid><description>MVCC 高并发情况, 应对数据冲突问题，一般两种方式:
见招拆招，避免冲突，使用 悲观锁 确保同时刻只能一个线程对数据进行修改。（Read/Write Locks、Two-Phase Locking) 开门迎客，允许冲突，但发生冲突的时候，要有能力检测到。（乐观锁）先认为不会发生冲突、除非检测到当前确实冲突了。（Logical Clock、MVCC ： Multi-Version Concurrent Control ） MVCC
核心思想: 一个数据多个历史版本，从而解决事务管理中数据隔离的问题。
版本 &amp;mdash;&amp;gt; 一般选择时间戳 或者 事务ID 标识
在处理写请求的时候， MVCC 是为数据添加一个新版本的数据；在读取数据的时候，先确定读取的版本，根据版本找到对应的数据。
https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc&amp;amp;q=
MYSQL中 MVCC innodb-multi-versioning
InnoDB 内部为每一行记录添加 三个隐藏的列，
DB_TRX_ID 创建记录/最后更新记录的事务ID,(删除也被视作更新，设置行的特殊位标记位删除) DB_ROLL_PTR 回滚指针，执行这条记录的上一个版本(rollback segment ： undo log，记录了更新前重建该记录的所需的信息) DB_ROW_ID 隐藏自增ID, 如果没有主键，就会以DB_ROW_ID 产生聚簇索引，随着插入新行，自增的， rollback segment 分为两种
inert undo log ，仅在事务回滚的时候需要，事务提交后立即丢弃 update undo log ，用于一致性读取，只有在 没有事务存在，且 头信息里面还有一个 delete_flag (bit) 标记为该记录是否删除
[待丰富]&amp;hellip;..
ETCD 中 MVCC mvcc
ETCD 使用 revision 作为一条记录的版本，revision 对于 etcd 的集群来说就像 逻辑时钟，每个数据发生更改做更新。</description></item><item><title>探秘负载均衡</title><link>https://pinkhello.cc/posts/36-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Fri, 01 Oct 2021 10:04:00 +0800</pubDate><guid>https://pinkhello.cc/posts/36-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>前言 今天是国庆的第3天，在此恭祝小哥哥和小姐姐们⛱️节日快乐！ 微信朋友圈🀄️全部是 旅途ING。
我的国庆：
第一天: 带🪆 第二天: 被启东坑了一回 第三天: 🈚️孩一身轻 &amp;hellip;&amp;hellip; 对于我这种社交尴尬者，⛱️度假最好的方式还是学习。
今日带来了 自己对于负载均衡(load balance)的学习。
度假最好的方式是学习 GRPC在负载均衡的设计: Load-balancing.md
为什么要负载均衡 在如今的互联网领域，对于数据的大爆发，高频的请求，对于提供服务的企业来说是不小的IT资源开销。为了解决资源中的负载分配，并且使得资源的利用率达到最大。出现了负载均衡，主要为了解决 高并发 和 高可用。
负载均衡的实现方式有：软件 和 硬件。这次我学习的主是软件方式。
负载均衡的算法 轮询( Round Robin ) &amp;amp; 加权轮询( Weight Round Robin ) : 随机 &amp;amp; 加权随机 最少连接( Last Connections ) : 通过将流量引导到第一个可用服务器然后将该服务器移动到队列底部来轮换服务器，当服务器具有相同的规格并且没有很多持久连接时最有用。 哈希( Hash ) eg: ip hash 一致性哈希( Consistent Hash ) eg: request_url 一致性哈希 最少响应时间 : 将流量定向到活动连接最少且平均响应时间最短的服务器。 常用的负载均衡手段 Load-balancing.md 已经介绍了以下几种
代理模式 代理模式需要可靠的并且能报告负载到负载均衡的系统的客户端，这种需要额外的资源来处理、并且负载均衡系统包含了每个RPC请求和响应的副本，增加了服务延迟
重客户端模式 这种方式、是绝大部分负载均衡的逻辑放置在客户端。( 可以从列表重选择服务器的负载均衡策略(循环、随机等)，服务器列表要么配置的、要么由其他服务提供、举个例子：比如 kafka、pulsar、redis 等等集群模式的客户端 ) 这种模式需要已多种语言的方式提供负载均衡策略、这种增加了客户端代码的复杂性。</description></item><item><title>短链接的思考与技术实现</title><link>https://pinkhello.cc/posts/35-%E7%9F%AD%E9%93%BE%E6%8E%A5%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0/</link><pubDate>Sun, 05 Sep 2021 00:19:13 +0800</pubDate><guid>https://pinkhello.cc/posts/35-%E7%9F%AD%E9%93%BE%E6%8E%A5%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0/</guid><description>前提 又到了一月一度的清理消息的时候了,开始清理短信,忽然见到一个幸福里的通知短信，那是在短信到达之前，本🐶打肿脸充胖子在幸福里小程序里面咨询上海的一套二手房的价格的，后来没关注， 这个短链接 https://m.xflapp.com/s/vdwecR 引起了本🐶的兴趣.
我发问：如何去实现一个短链接服务?
短链接的原理 短链接的核心是什么?
构建的短链接和长链接(我们真实的访问地址)的唯一关联映射、也就是映射标识生成算法。
从这个定义也能得到短链接服务的几个特点:
1⃣️具有高性能 2⃣️是排列组合数量量要足够大 3⃣️要具有不易破解、且破解难度极大 拆解短链接 👆的短链接 https://m.xflapp.com/s/vdwecR 我掏出了我的神器 Chrome 打开了一下，直接跳转到了http://m.haoduofangs.com/f100/activity/client/tt_im?app_id=13&amp;amp;customer_user_id=62800686641&amp;amp;realtor_id=3781951248937422. 好了仔细看一下请求和响应,响应的302,响应头中带了需要跳转的地址.
综上我示意图:
可以看出来,构建唯一映射关系是将原有的固定长链接生成一个或多个短链接,1:N关系。生成的短链接必须满足:
不能轻易的被猜出来(破解),被恶意遍历。 [一定时间内]不能重复(一个短链接在一定时间内只能对应一个长链接、这个一定时间可以是永久) 长度尽量短 在短信运营商会限制短信的长度、如果太长、留给运营人员或者客户的的字符长度会有🚫，这不是给同事或客户挖坑么。 链接变二维码，二维码的点位会非常密集，不利于客户端识别和传输 长度太长，从个人情感里面也比较繁琐这样的长链接，不易记录和输入 回到这个唯一映射关系上来，就是需要对这个长链接做 Hash，和大多数哈希算法一致,生成算法需要就是要具备唯一性和较低的碰撞率的特点,而在短链接场景下，又决定了它还要具备短线精悍并且易于传输的特点。
短链接压缩生成算法特点:
短小精悍、易于传输 高度唯一性 低碰撞率 短链接生成算法 从上图可以看出，协议、域名均为不可变部分，能定制的只有压缩码(哈希码), 那么作为程序🐶能玩的也只有这部分了。
回到幸福里通知短信的短链接, 它抛掉协议和域名两部分,还剩下s/vdwecR, 可以看出Path其实是两层,第一层是s,第二层是vdwecR， 可以抽象为{pathLevel1}/{pathLevel2},{pathLevel1}为了简短使用了单个字母(可以区分大小写)或者数字, {pathLevel2} 为了满足低碰撞率和唯一性，使用了6位区分大小写字母和数字的组合体。 那么像这个组合体的最大组合数量是多少呢？{pathLevel1} 假设只有一位, {pathLevel2} 有6位,从当前的例子来看，不考虑取值为数字的可能，那么它的组合数 每个取值都是(26个大写字母 + 26个小写字母) ，也就是 52 ^ 7 = 1,028,071,702,528 ,已达万亿级别数量级。
现在我做个假设，我们只实现有一个级别的path, 每个字符串的取值为(26个大写字母 + 26个小写字母 + 10 个数字)，那么这个组合数就看位数N了: 62 ^ N 为组合数数量级
N=4 62^4 = 14,776,336 达到 147.</description></item><item><title>Aws上mongo备份进入S3</title><link>https://pinkhello.cc/posts/34-aws%E4%B8%8Amongo%E5%A4%87%E4%BB%BD%E8%BF%9B%E5%85%A5s3/</link><pubDate>Fri, 27 Aug 2021 17:58:15 +0800</pubDate><guid>https://pinkhello.cc/posts/34-aws%E4%B8%8Amongo%E5%A4%87%E4%BB%BD%E8%BF%9B%E5%85%A5s3/</guid><description>机器准备 开启新机器, 注意需要备份的数据量。选择磁盘或者数据盘大小 老机器上, 磁盘在线需要扩容的话注意执行扩容，让设备分区和文件系统都识别 云台上选择需要扩容的磁盘扩大到需要的大小（当前机器上还无法直接使用）
进入机器控制台
# lsblk 查看磁盘大小 # lsblk -f 查看磁盘大小以及类型 # df -lh # lsblk # sudo growpart 磁盘设备名称 1 # sudo resize2fs 磁盘文件系统 # df -lh df -lh lsblk lsblk -f sudo growpart /dev/nvme0n1 1 sudo resize2fs /dev/nvme0n1p1 备份脚本准备 backup.sh 备份脚本 #!/bin/bash # 公共 function 日志日期格式 get_date () { date +[%Y-%m-%d\ %H:%M:%S] } ARCHIVE_OUT=$BACKUP_FILENAME_PREFIX-$(date +$BACKUP_FILENAME_DATE_FORMAT).tgz echo &amp;#34;ARCHIVE_OUT=$ARCHIVE_OUT&amp;#34; # 生成POST请求数据 generate_post_data () { cat &amp;lt;&amp;lt;EOF { &amp;#34;group&amp;#34;:&amp;#34;alert-XXX&amp;#34;, &amp;#34;project&amp;#34;:&amp;#34;$S3_PATH$ARCHIVE_OUT&amp;#34;, &amp;#34;alert_message&amp;#34;:&amp;#34;备份mongo2s3错误&amp;#34; } EOF } # Script echo &amp;#34;$(get_date) Mongo backup started&amp;#34; echo &amp;#34;$(get_date) [Step 1/4] Running mongodump: mongodump --forceTableScan -h $MONGO_HOST $MONGO_DB_ARG -u $MONGO_USERNAME -p $MONGO_PASSWORD --authenticationDatabase admin&amp;#34; # mongodump --quiet -h $MONGO_HOST:$MONGO_PORT $MONGO_DB_ARG -u $MONGO_USERNAME -p $MONGO_PASSWORD --authenticationDatabase admin mongodump --forceTableScan -h $MONGO_HOST $MONGO_DB_ARG -u $MONGO_USERNAME -p $MONGO_PASSWORD --authenticationDatabase admin echo &amp;#34;$(get_date) [Step 2/4] check dump directory: curl feishu .</description></item><item><title>RocketMQ源码阅读 Consumer</title><link>https://pinkhello.cc/posts/28-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-consumer/</link><pubDate>Sat, 31 Jul 2021 15:10:43 +0800</pubDate><guid>https://pinkhello.cc/posts/28-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-consumer/</guid><description>RocketMQ Consumer 分为两类,
DefaultMQPullConsumer 标记为弃用 DefaultMQPushConsumer 我们这次只分析 DefaultMQPushConsumer
当前先看个 PushConsumer 代码
public class PushConsumer { public static void main(String[] args) throws InterruptedException, MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&amp;#34;CID_JODIE_1&amp;#34;); /** * 设置订阅的 可以对指定消息进行过滤，例如：&amp;#34;TopicTest&amp;#34;,&amp;#34;tagl||tag2||tag3&amp;#34;,*或null表示topic所有消息 */ consumer.subscribe(&amp;#34;TopicTest&amp;#34;, &amp;#34;*&amp;#34;); /** * CONSUME_FROM_LAST_OFFSET 第一次启动从队列最后位置消费，后续再启动接着上次消费的进度开始消费 * CONSUME_FROM_FIRST_OFFSET 第一次启动从队列初始位置消费，后续再启动接着上次消费的进度开始消费 * CONSUME_FROM_TIMESTAMP 第一次启动从指定时间点位置消费，后续再启动接着上次消费的进度开始消费 */ consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); //wrong time format 2017_0422_221800 consumer.setConsumeTimestamp(&amp;#34;20181109221800&amp;#34;); /** * 注册并发消费（消费监听） */ consumer.registerMessageListener(new MessageListenerConcurrently() { /** * ConsumeConcurrentlyStatus.CONSUME_SUCCESS 成功消费 * ConsumeConcurrentlyStatus.RECONSUME_LATER broker会根据设置的messageDelayLevel发起重试，默认16次 * @param msgs * @param context * @return */ @Override public ConsumeConcurrentlyStatus consumeMessage(List&amp;lt;MessageExt&amp;gt; msgs, ConsumeConcurrentlyContext context) { System.</description></item><item><title>RocketMQ源码阅读 Producer</title><link>https://pinkhello.cc/posts/27-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-producer/</link><pubDate>Fri, 23 Jul 2021 15:10:37 +0800</pubDate><guid>https://pinkhello.cc/posts/27-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-producer/</guid><description>在消息发送的时候, 我们先看 Producer 的启动代码
public class Producer { public static void main(String[] args) throws MQClientException, InterruptedException { DefaultMQProducer producer = new DefaultMQProducer(&amp;#34;ProducerGroupName&amp;#34;); producer.start(); for (int i = 0; i &amp;lt; 128; i++){ try { Message msg = new Message(&amp;#34;TopicTest&amp;#34;, &amp;#34;TagA&amp;#34;, &amp;#34;OrderID188&amp;#34;, &amp;#34;Hello world&amp;#34;.getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.send(msg); System.out.printf(&amp;#34;%s%n&amp;#34;, sendResult); } catch (Exception e) { e.printStackTrace(); } } producer.shutdown(); } } 非常简单的代码构建了一个 Producer 下面肯定有疑问:
Producer 如何启动的 Producer 发送消息流程 Producer 和 NameSever 如何交互 Producer 发送消息如何保证顺序 Producer 负载均衡如何实现的 Producer 延迟消息（发送的时候设置 Message 的延迟级别） Producer 端 如何启动的 第一步 可以看到启动一个 Producer 的代码, 核心类 DefaultMQProducer ,构造方法中又 New 了一个新的类 DefaultMQProducerImpl, 这个类是给真正的核心的类</description></item><item><title>RocketMQ源码阅读 NameServer</title><link>https://pinkhello.cc/posts/26-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nameserver/</link><pubDate>Tue, 25 May 2021 19:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/26-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nameserver/</guid><description>NameServer 角色 NameSever 在 RocketMQ 起到重要的角色，承担着路由管理、服务注册、服务发现等核心功能。
接收 Broker 的请求注册 Broker 路由信息 接收 Client 请求根据某个 topic 获取所有到 broker 的路由信息 NameSrv 核心类 NamesrvStartup public class NamesrvStartup { //... public static NamesrvController main0(String[] args) { try { NamesrvController controller = createNamesrvController(args); //启动 NamesrvController start(controller); String tip = &amp;#34;The Name Server boot success. serializeType=&amp;#34; + RemotingCommand.getSerializeTypeConfigInThisServer(); log.info(tip); System.out.printf(&amp;#34;%s%n&amp;#34;, tip); return controller; } catch (Throwable e) { e.printStackTrace(); System.exit(-1); } return null; } //... /** * 启动 NamesrvController * @param controller * @return * @throws Exception */ public static NamesrvController start(final NamesrvController controller) throws Exception { if (null == controller) { throw new IllegalArgumentException(&amp;#34;NamesrvController is null&amp;#34;); } // NamesrvController 初始化 boolean initResult = controller.</description></item><item><title>RocketMQ源码阅读 通信组件</title><link>https://pinkhello.cc/posts/25-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E9%80%9A%E4%BF%A1%E7%BB%84%E4%BB%B6/</link><pubDate>Sat, 22 May 2021 16:09:45 +0800</pubDate><guid>https://pinkhello.cc/posts/25-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E9%80%9A%E4%BF%A1%E7%BB%84%E4%BB%B6/</guid><description>RocketMQ 核心基石 前面已经介绍了 RocketMQ 的基本的概念和组件。今天我们开启真正的源码的阅读诗篇, RocketMQ 消息系各个组件 Producer、Consumer、Broker、NameSrv 通通离不开交互，那是使用的什么交互的呢。答案是TCP长链接。 而 RocketMQ 开源代码内部，对通信相关的进行了一次封装，都在 rocketmq-remoting 模块下，这个模块被其他 client、broker、namesrv 应用。
直接先说 remoting 的实现是基于 netty 做了封装、启动了服务端和客户端，支持三种消息的发送方式:
同步发送 单向发送 (不需要关注响应) 异步发送 下图为异步通信流程 remoting 包下的核心接口体系 接口 RemotingService public interface RemotingService { // 开启 void start(); // 关闭 void shutdown(); // 注册 RPCHook void registerRPCHook(RPCHook rpcHook); } 接口 RemotingServer public interface RemotingServer extends RemotingService { // 注册请求类型的处理器 【common 模块的 org.apache.rocketmq.common.protocol.RequestCode] void registerProcessor(final int requestCode, final NettyRequestProcessor processor, final ExecutorService executor); // 注册默认的处理器 void registerDefaultProcessor(final NettyRequestProcessor processor, final ExecutorService executor); // 本地的端口 int localListenPort(); // 根据 requestCode 获取处理器和业务线程池 Pair&amp;lt;NettyRequestProcessor, ExecutorService&amp;gt; getProcessorPair(final int requestCode); // 同步发送 RemotingCommand invokeSync(final Channel channel, final RemotingCommand request, final long timeoutMillis) throws InterruptedException, RemotingSendRequestException, RemotingTimeoutException; // 异步发送 void invokeAsync(final Channel channel, final RemotingCommand request, final long timeoutMillis, final InvokeCallback invokeCallback) throws InterruptedException, RemotingTooMuchRequestException, RemotingTimeoutException, RemotingSendRequestException; // 单向发送 void invokeOneway(final Channel channel, final RemotingCommand request, final long timeoutMillis) throws InterruptedException, RemotingTooMuchRequestException, RemotingTimeoutException, RemotingSendRequestException; } 实现 NettyRemotingServer 这边选择性的进行摘取记录描述啊</description></item><item><title>RocketMQ源码阅读 开篇</title><link>https://pinkhello.cc/posts/24-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E5%BC%80%E7%AF%87/</link><pubDate>Thu, 20 May 2021 10:02:20 +0800</pubDate><guid>https://pinkhello.cc/posts/24-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E5%BC%80%E7%AF%87/</guid><description>RocketMQ 是什么? RocketMQ 是 Alibaba 捐赠给 Apache 的一款分布式、队列模型的开源消息中间件。
Github https://github.com/apache/rocketmq 从官网也能看出它的一些特性:
低延迟 高可用 万亿级的消息支持 &amp;hellip; RocketMQ 基本概念 RocketMQ 是由 Producer、Broker、Consumer 三部分组成, Producer 负责生产 Message, Consumer 负责消费 Message, Broker 负责存储 Message。 每个 Broker 可以存储多个 Topic 的消息, 每个 Topic 的消息也可以分片存储在不同的 Broker 上。 Message Queue 用于存储消息的物理地址，每个 Topic 的消息地址存储于对歌 Message Queue 中。 Consumer Group 由多个 Consumer 实例组成。
Producer 负责生产消息，同步发送、异步发送、顺序发送、单向发送。同步和异步需要 Broker 确认信息，单向发送不需要。
Consumer 负责消费消息，一般异步消费。一个消费者会从 Broker 拉取消息。（拉取式消费、推动式消费）
Broker Server 负责存储、转发消息。 接收 Producer 发送来的消息并存储、同时为 Consumer 拉取请求做准备。当然也存储这消息相关的元数据（消费组、消费进度偏移、主题、队列消息等）
Name Server 为消息路由的提供者。Producer 和 Consumer 能够通过 它查找各个 Topic 相应的 Broker IP 列表，多个 Name Server 组成集群，相互独立、没有信息交换。</description></item><item><title>回望K8S 持久化存储</title><link>https://pinkhello.cc/posts/22-%E5%9B%9E%E6%9C%9Bk8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</link><pubDate>Tue, 18 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/22-%E5%9B%9E%E6%9C%9Bk8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</guid><description>PV、PVC、StorageClass 说的啥？ PV: 持久化存储数据卷，这个 API 主要定义的是一个持久化存储在宿主机上的一个目录。一般由运维人员进行定义，比如定义一个 NFS 类型的 PV
apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.5 path: &amp;#34;/&amp;#34; PVC: POD 所希望使用的持久化存储的属性. 比如 Volume 的存储大小、可读写权限等. PVC 一般由开发人员创建、或者由 PVC模板的方式成为StatefulSet的一部分，由StatefulSet控制器负责创建带编号的PVC.
# 创建一个 1 GB 大小的PVC --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi ... 用户创建的PVC要真正的被容器使用的化，需要于心和某个符合条件的PV进行绑定：
第一个条件，PV 和 PVC 的 spec 字段。例如: PV 的存储(storage)大小就必须满足 PVC 的要求 第二个条件，PV 和 PVC 的 storageClassName 字段名称必须一样。 下面是去使用这个PVC</description></item><item><title>回望K8S 容器编排与Kubernetes作业管理</title><link>https://pinkhello.cc/posts/20-%E5%9B%9E%E6%9C%9Bk8s-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E4%B8%8Ekubernetes%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 17 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/20-%E5%9B%9E%E6%9C%9Bk8s-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E4%B8%8Ekubernetes%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86/</guid><description>Pod pod 是 Kubernetes 项目的最小的 API 对象，原子调度单位.
假设 &amp;ldquo;容器的本质是进程&amp;rdquo;，容器镜像就是 exe 安装包, kubernetes 是操作系统
Pod 最重要的一个事实是一个逻辑概念。它对于 Kubernetes 最核心的意义是 容器设计模式。Kubernetes 真正处理的还是宿主机上操作系统上的 Linux 容器的 Namespace 和 Cgroups，而不是一个所谓的 Pod 边界和隔离环境。
Pod 其实是一组共享了某些资源的容器。Pod 里面所有的容器，共享的同一个 Network Namespace，并且可以声明共享同一个 Volume.
Kubernetes 项目内部，Pod 实现需要使用一个中间容器，这个容器叫做 Infra 容器，在 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。
sidecar 典型的例子：WAR 包和 Web 服务器 POD后，将 WAR 包和 Tomcat 分别做成镜像，可以把他们容器结合在一起
--- apiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: # 启动后做了一件事 把应用的WAR包拷贝到 /app目录中，后退出 initContainers: - image: sample-war:v2 name: war command: [&amp;#34;cp&amp;#34;, &amp;#34;/sample.</description></item><item><title>回望K8S Kubernetes拼图</title><link>https://pinkhello.cc/posts/19-%E5%9B%9E%E6%9C%9Bk8s-kubernetes%E6%8B%BC%E5%9B%BE/</link><pubDate>Sun, 16 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/19-%E5%9B%9E%E6%9C%9Bk8s-kubernetes%E6%8B%BC%E5%9B%BE/</guid><description>kubernetes 安装 all 节点安装 Docker 和 Kubeadm 所有节点 root 用户下操作
&amp;gt; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &amp;gt; cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF &amp;gt; apt-get update # 这一步安装的时候 kubeadm 和 kubelet、kubectl、kubernetes-cni 都会自动安装完毕 &amp;gt; apt-get install -y docker.io kubeadm 提示：如果 apt.kubernetes.io 因为网络问题访问不到，可以换成中科大的 Ubuntu 镜像源 deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main。
部署 Kubernetes Master 声明一个 kubeadm.yaml
--- apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration controllerManagerExtraArgs: # 配置了自定义自动水平扩展 horizontal-pod-autoscaler-use-rest-clients: &amp;#34;true&amp;#34; horizontal-pod-autoscaler-sync-period: &amp;#34;10s&amp;#34; node-monitor-grace-period: &amp;#34;10s&amp;#34; apiServerExtraArgs: runtime-config: &amp;#34;api/all=true&amp;#34; # kubeadm 部署的 kubernetes 的版本 kubernetesVersion: &amp;#34;stable-1.</description></item><item><title>回望K8S 白话容器</title><link>https://pinkhello.cc/posts/18-%E5%9B%9E%E6%9C%9Bk8s-%E7%99%BD%E8%AF%9D%E5%AE%B9%E5%99%A8/</link><pubDate>Sat, 15 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/18-%E5%9B%9E%E6%9C%9Bk8s-%E7%99%BD%E8%AF%9D%E5%AE%B9%E5%99%A8/</guid><description>进程开启 容器, 到底是什么? 前面提出: 容器是一种沙盒技术. 就是一个集装箱, 把应用装起来的技术. 这样, 应用与应用之间有了边界不至于互相干扰; 有了这些集装箱, 也方便搬来搬去.
码农都知道可执行的二进制文件是代码的可执行镜像(executable image). 一旦程序执行起来, 内存数据、寄存器的值、堆栈的指令、打开的文件等这些集合汇集成一个程序的计算机执行环境总和: 进程.
进程: 静态表现是程序, 动态表现计算机的数据和状态的总和。
容器的核心功能, 就是通过约束和修改进程的动态表现, 从而为其创造一个&amp;quot;边界&amp;quot;.
Cgroups 技术 制造约束的主要手段 Namespace 技术 修改进程视图的主要方法 docker run , -it 告诉 Docker 启动容器后, 需要分配一个文本输入/输出环境, 也就是 TTY, 跟容器的标准输入相关联, 这样我们就可以和这个Docker容器进行交互了。而 /bin/sh 就是我们在 Docker 容器里运行的程序.
&amp;gt; docker run -it busybox /bin/sh / # 帮我启动一个容器, 在容器里执行 /bin/sh, 并且给我分配一个命令行终端跟这个容器进行交互, 在这个执行环境下可以完全执行LINUX命令,且与宿主机完全隔离在不同的世界中.
Docker对被隔离应用的进程空间做了手脚, 使得这些进程只能看到重新计算的进程编号, 可是实际上, 他们在宿主机的操作系统里, 还是原来的第N号进程. 这种技术就是Linux内部的Namespace机制。
Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如：</description></item><item><title>回望K8S 小鲸鱼容器技术</title><link>https://pinkhello.cc/posts/17-%E5%9B%9E%E6%9C%9Bk8s-%E5%B0%8F%E9%B2%B8%E9%B1%BC%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/</link><pubDate>Fri, 14 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/17-%E5%9B%9E%E6%9C%9Bk8s-%E5%B0%8F%E9%B2%B8%E9%B1%BC%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/</guid><description>什么是容器 在容器之前, 火爆云计算市场的是 PAAS, PAAS已经深入人心. 那时候突然有一家公司 dotCloud 剑走偏锋, 直接开源出了 Docker 项目，并且直接面向的社区。 这样的做法直接将当时的PAAS流主要公司打的屁滚尿流。
回头看, PAAS 最核心的是隔离环境,或者叫 沙盒,在我看来也就是 容器. 而 Docker 项目和 Cloud Foundry 的容器没有太大的不同,但是它为什么能针对 PAAS进行了一场快速的闪电战呢？
对的, 就是 Docker 镜像, 这个小小的创新, 迅速改变了云计算的发展轨迹! Docker 镜像解决的是 打包 问题。也许有人说Docker 镜像就是一个压缩包。但是就是这个压缩包包含了完整的操作系统文件和目录, 包含了整个应用所需要的依赖，一包在手, 你可以轻易的运行你的沙盒,并且本地环境与云端环境高度一致（这是最宝贵的）。
Docker给PAAS进行了致命打击, 提供了便利的打包机制, 面向后端开发者来说, 屏蔽了机器、内核等技术细节, 避免了在不同环境间的差异引入的试错成本。是一次解放生产力的革命。当然很多开发者用脚投票, 了结了PAAS时代。
Docker 三大利器 Docker项目的高调开源, 解决了打包和发布困扰运维的技术难题，同时它也第一次纯后端的概念通过友好的设计和封装交付到了开发者的手里。 Swarm,Docker是创建和启停容器的工具,那么Swarm是为了向平台化发展而提出的。它提供了完整的整体对外提供集群管理功能,它的亮点是完全使用Docker原本的管理容器的API来完成集群管理 # Swarm多机环境下，指令会被Swarm拦截处理，后面通过调度算法找到合适的Docker Daemon运行 docker run -H &amp;#34;Swarm集群API&amp;#34; &amp;#34;我的容器&amp;#34; Compose(Fig)项目, 这是第一次在开发者面前提出 容器编排(Container Orchestration)概念。 应用容器 A, 数据库容器B, 负载均衡容器C, Compose 允许 A、B、C 三个容器定义在配置文件中, 并指定关联关系. 只需要执行 (fig/docker-compose up)</description></item><item><title>Hexo迁移Hugo</title><link>https://pinkhello.cc/posts/16-hexo%E8%BF%81%E7%A7%BBhugo/</link><pubDate>Tue, 11 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/16-hexo%E8%BF%81%E7%A7%BBhugo/</guid><description>为什么迁移 Hugo Hugo 使用比 Hexo 简单, 只有单独的一个二进制文件 苦于 Hexo 的 NodeModule 管理 迁移成本更低, 结合 Github Action 实现 Markdown 文章发布, 自动更新至静态站 规划：加入自定义域名以及做静态资源CDN做的加速 前置工作 1、 之前基本所有的博客都托管与 github,这次也不例外, 复用 https://pinkhello.github.io,创建两个项目
pinkhello.github.io template 仓库 pinkhello.github.io.source private 仓库 2、准备OpenSSH私钥和公钥
pinkhello.github.io 仓库 添加 settings -&amp;gt; Deploy keys -&amp;gt; Add Deploy Key (将公钥添加进去、注意允许 Write) pinkhello.github.io.source 仓库 添加 settings -&amp;gt; Actions secrets -&amp;gt; New Repository Secret ( NAME : ACTION_DEPLOY_KEY, Value: 私钥 ) 3、git clone pinkhello.github.io.source 仓库
git clone git@github.</description></item><item><title>记一次docker日志磁盘告警问题</title><link>https://pinkhello.cc/posts/15-%E8%AE%B0%E4%B8%80%E6%AC%A1docker%E6%97%A5%E5%BF%97%E7%A3%81%E7%9B%98%E5%91%8A%E8%AD%A6%E9%97%AE%E9%A2%98/</link><pubDate>Mon, 10 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/15-%E8%AE%B0%E4%B8%80%E6%AC%A1docker%E6%97%A5%E5%BF%97%E7%A3%81%E7%9B%98%E5%91%8A%E8%AD%A6%E9%97%AE%E9%A2%98/</guid><description>前景 今日，我正在开开心心的刷着JFX的Coding中，突然线上报警群中爆了个炸弹，EC2磁盘超过80%。
处理过程 解决问题姿势就位：
赶紧开机 ==》 ❤️中万匹🦙奔腾而过 ❤️中MMP
默默的通过跳板机进入目标机器
不管三七二十一,执行查看磁盘占用大小，我的乖乖，占用确实超过了87%了，一下子暴涨的
# 查看磁盘占用大小 &amp;gt; sudo df -h # 查看当前目录总量 &amp;gt; sudo du -sh 开始定位具体哪个文件或者目录占用这么大,跑到根目录下。 # 查看当前目录下一级子文件和子目录占用的磁盘容量 &amp;gt; sudo du -lh --max-depth=1 一开始猜想可能是docker容器的日志占用大，上面执行后，还真 TM 是 /var/lib/docker/containers 目录占用 42G 开始查看是哪个容器占用的这么大的空间 # 查看 containers 日志目录排序 &amp;gt; sudo du -d1 -h /var/lib/docker/containers | sort -h # 查看具体的哪个日志文件大 &amp;gt; sudo find /var/lib/docker/containers -name *.log 当然这个配图是我清理之后的 定位到最大的文件，一顿操作 # 清空比较大的日志文件 &amp;gt; sudo sh -c &amp;#34;cat /dev/null &amp;gt; ${log_file}&amp;#34; 思考 上面的方式是一种方式解决【临时】 # 查看 docker 的 Logging Driver &amp;gt; docker info | grep &amp;#39;Logging Driver&amp;#39; 如何彻底解决这个问题： 写个shell脚本 使用 crontab 定期执行清理 #!</description></item><item><title>工作纪实2020</title><link>https://pinkhello.cc/posts/14-%E5%B7%A5%E4%BD%9C%E7%BA%AA%E5%AE%9E2020/</link><pubDate>Wed, 05 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/14-%E5%B7%A5%E4%BD%9C%E7%BA%AA%E5%AE%9E2020/</guid><description>每日一思篇 [2019-10-12 每日一思] Mysql WAL技术 和 RingBuffer 思想好一致? [2019-10-14 每日一思] JWT 续签该如何做? [2019-10-16 每日一思] TCP/IP 协议具体指哪些? 我们都知道网络是7层模型，应表会传网数物， 现在我只讨论应传网数这4层。TCP/IP协议应该被称为TCP/IP族， 我的理解他不是属于单个的协议类型，是一个统称，知道网络模型核心设计思想是分层，为什么分层，分层从设计上和实现难度上都简单很多，哪一层需要修改只需要修改这一层。
应用层，像最常见的http、ftp、dns、rtsp、rtmp等等协议都是属于这类， 传输层呢按照传输类型又分了TCP和UDP, 网络层，是数据包交互的层面， 数据链路层是处理网卡、操作系统等等软硬抽象出的可见部分。 举一个栗子，一个http请求，在应用层面是完整的，后面被传输层（TCP层）被分包，并打上序号标记，再进入网络层（IP层）添加IP首部（目标mac地址等等）， 下面就是开始疯狂的发送了，接收方一样是这个过程的逆序。应用处理完成后面的响应过程与请求过程一样的一个过程。同时可以扩展出L4与L7的问题， 各自是如何去实现负载均衡的？L4是可以看出是基于传输层即TCP层工作（通过发布VIP（第三层）以及第四层端口），L7基于应用层工作（第四层基础上+考虑应用特征）， 比如HTTP的URL、客户端的类别、语言类型等等。
[2019-10-18 每日一思]一种场景，rabbitmq 的 Exchange 为 fanout 类型，绑定到多个queue, 什么情况会触发 rabbitmq 流控？如何解决？ [2019-10-22 每日一思]ID序列生产器怎么实现呢？ uuid生成
基于时间（60位utc时间 和 时间序列值14位，以及mac地址） 基于名称（针对命名空间dns、url等分配，把名称转成字节序列，再用md5或sha-1与命名空间标识进行计算，产生哈希结果） 基于随机数（密码学随机数，系统的硬盘内存线程堆栈进程句柄等sha-1生成哈希结果） snowflake，64bit，long型ID
ID生成方式 1bit（不使用），41bit时间戳（当前毫秒数、69年一轮回），10bit机器码（1024台，5bit数据中心，5bit机器ID），12bit作为毫秒内序列号（单机理论 409.6w/s） 雪花算法，多台机器，有因为时钟回拨导致的ID生成问题，当然可以通过发生时钟回拨后一个阈值，在阈值内则不允许产生新的ID，同步阻塞，在阈值外重新设置机器ID来解决 github.com/baidu/uid-generator 技术老铁百度开源的基于snowflake实现的ID生成器，可以借鉴研读一下 [2019-11-01 每日一思]我们常说的限流是什么？为什么要限流？限流有哪些方式？ 我们常说的限流，顾名思义即限制流量. 限制系统的输入和输出 常用的限流发展至今，有四种方式
固定时间计数器 漏桶 令牌桶 滑动窗口计数器 固定窗口计数器：以单位时间内进入系统（系统级别）或者某一个单一接口服务（系统服务级别）请求次数，在这个单位时间内的超过次数，拒绝服务或者更换其他方案（降级、熔断）达到限流目的。
可以看出，明显的缺点，从整体曲线上，毛刺现象非常严重，假设单位时间 1s 内限制 100 次，在0-10ms内我已经请求超过100次了，后面的请求全部拒绝或者做其他处理了。无法控制单位时间内的突发流量。
漏桶: 桶的容量固定，桶流出的速率恒定。桶满则限流。也是无法应对突发流量
令牌桶： 还是桶的方式， 桶中存放的是 token ，根据限流的大小， token 以恒定速率进入桶中，设置桶的最大的 token 容量，当桶满时候拒绝新添加的token ，或者直接丢弃。所有请求进入先获取令牌，得到令牌继续下面的业务逻辑，处理完成删除 token。</description></item><item><title>Kafka与Debezium构建CDC管道</title><link>https://pinkhello.cc/posts/13-kafka%E4%B8%8Edebezium%E6%9E%84%E5%BB%BAcdc%E7%AE%A1%E9%81%93/</link><pubDate>Tue, 04 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/13-kafka%E4%B8%8Edebezium%E6%9E%84%E5%BB%BAcdc%E7%AE%A1%E9%81%93/</guid><description>建设篇 1、什么是 debezium? https://debezium.io/
Tutorial https://debezium.io/documentation/reference/1.3/tutorial.html
2、Debezium 如何工作的 2.1 Debezium 支持的数据库类型 MySQL MongoDB PostgreSQL Oracle SQL Server Db2 Cassandra 2.2 Debezium 三种方式运行 Kafka Connect Debezium Server Embedded Engine https://github.com/debezium/debezium-examples/tree/master/kinesis
3、在 K8S 中构建基础Debezium集群环境 镜像准备
kafka | debezium https://hub.docker.com/r/debezium/kafka zookeeper | debezium https://hub.docker.com/r/debezium/zookeeper connect | debezium https://hub.docker.com/r/debezium/connect schema-registry | confluentinc https://hub.docker.com/r/confluentinc/cp-schema-registry ps： debezium 参考地址 https://github.com/debezium/docker-images confluentinc 参考地址 https://github.com/confluentinc/cp-all-in-one/tree/latest/cp-all-in-one
3.1 K8S基础知识 kafka 与 zookeeper 建设为 stateful 状态集群 schema-registry 主要为了 支持 avro 格式这些不需要写到 kafka 消息头里面，减少消息的大小，额外的服务，属于 kafka 生态，存储依赖 kafka broker保证稳定性。 k8s steteful 集群 0&amp;hellip;~ n 个 POD zookeeper 里面 zoo.</description></item><item><title>老司机聊聊BatChat</title><link>https://pinkhello.cc/posts/12-%E8%81%8A%E8%81%8A%E8%9D%99%E8%9D%A0chat/</link><pubDate>Mon, 03 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/12-%E8%81%8A%E8%81%8A%E8%9D%99%E8%9D%A0chat/</guid><description>今天，在浏览小众软件的时候，突然看见一个早期小编的推广～～～蝙蝠APP，联想到之前新闻中看到的有个贩卖个人信息的人说在蝙蝠APP上，勾起了我的好奇心，作为一个程序🐶，默默的想探究一下这个蝙蝠APP玩的什么套路。
什么是蝙蝠（BatChat） 一款免费的端到端加密的蝙蝠APP，随时畅聊！ 应用 IOS、Android https://batchat.com
它的特性 安全 ｜ 畅聊时、端到端加密（所有消息经过端到端加密，任何聊天记录不进行云存储，让你的信息想象中更安全） 隐私 ｜ 畅聊时、双向撤回（聊天记录一键双向撤回，同时删除你和对方设备上所有的聊天记录，撤回数据多次覆盖删除、不可恢复、保证双方的隐私安全） 匿名群聊 ｜ 开启匿名群聊，群里面的每一个成员都可以&amp;quot;变身&amp;quot;，隐藏真实身份，群内不受身份约束 什么是端到端加密？ 端到端加密是在源结点和目的结点中对传送的数据进行加密和解密， 因此数据的安全性不会因中间结点的不可靠而受到影响。 蝙蝠APP的所有数据都通过用户端生成的私钥进行加密后再发送，任何第三方包括开发者都不能解开此数据.
蝙蝠APP采用的安全层级？ 通道加密 通道加密中采用了哪些加密算法？ 通道加密中使用到的 RSA, ECDHE, AES256_CBC, SHA256, SHA1等等(如下图)。
步骤:
(1. 客户端和服务器先产生随机数 (2. 服务器下发随机数。 (3. 客户端用 RSA 对客户端随机数进行加密，并发送给服务器。 (4. 服务器用 RSA 解密客户端随机数（使用 RSA 的目的是防止中间人攻击）。 (5. 客户端，服务器用自己的随机数加上对方的随机数生成临时密钥 TempKey 和临时偏移量 TempIV (此时双方均持有相同的 TempKey和TempIV)。 (6. 客户端服务器均使用 ECDHE 生成各自的公私钥对。 (7. 客户端用 TempKey, TempIV 对自己的 ECDHE 公钥进行加密，并将密文发送给服务器。 (8. 服务器收到客户端的 ECDHE 公钥密文，并解密。 (9. 服务器使用 TempKey, TempIV 对自己的 ECDHE 公钥进行加密，并使用 RSA 对 ECDHE。 (10.</description></item><item><title>几个关于kafka的知识点</title><link>https://pinkhello.cc/posts/11-%E5%87%A0%E4%B8%AA%E5%85%B3%E4%BA%8Ekafka%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/</link><pubDate>Sun, 02 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/11-%E5%87%A0%E4%B8%AA%E5%85%B3%E4%BA%8Ekafka%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/</guid><description>认识kafka Kafka 是分布式消息系统， Apache 的子项目。标语也变了&amp;quot;分布式流平台&amp;quot;， 与传统的消息系统不同点在于
分布式的，易于扩展 为发布和订阅提供了高吞吐 支持多订阅者，在失败的时候能自动平衡消费者 消息的持久化 kafka 的架构 几点？
Kafka 的 Topic 和 Partition 内部如何存储？ 与传统的消息系统相比， Kafka 消费模型有啥优点？ Kafka 是如何实现分布式数据存储和数据的读取？ Kafka 架构 一个 Kafka 集群，多个 Producer ，多个 Consumer ，多个 Broker ， 选举 Leader 以及在 Consumer Group 发生变化时进行 reblance 。
Broker 消息中间件的处理节点，一个 Kafka 节点就是一个 Broker ， 一个或者多个 Broker 组成 Kafka 集群 Topic Kafka 根据 Topic 对 Message 进行归类，发布到 Kafka 的每条 Message 都要指定 Topic Producer 向 Broker 发生 message Consumer 从 Broker 读取 message Consumer Group 每个 Consumer 属于特定的 Group，一个 Message 可以发送给不同的 Consumer Group ，但是同一个 Group 下的只有一个 Consumer 能消费该 Message Partition 物理概念，一个 Topic 下可以分为多个 Partition, 每个 Partition 下是有序的。 下面来讲述 上面为问题啊</description></item><item><title>多域名下的SSH</title><link>https://pinkhello.cc/posts/10-%E5%A4%9A%E5%9F%9F%E5%90%8D%E4%B8%8B%E7%9A%84ssh/</link><pubDate>Fri, 02 Apr 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/10-%E5%A4%9A%E5%9F%9F%E5%90%8D%E4%B8%8B%E7%9A%84ssh/</guid><description>前言 有时候我们，有多个 git 账号（Gitlab、GitHub），这时候如果是同一个账号（邮箱注册），那不会有问题，但是如果不是相同的账号呢，我们在使用 SSH KEY 做免密登录时候，头痛了。
这个时候我们需要针对不同的账号，生成不同的 SSH Key，并且配置不同的域名使用不同的Key
生成一个 SSH KEY ssh-keygen -t rsa -C &amp;#34;username@email.com&amp;#34; 一路 Enter，并且在生成时候指定名字，（不指定名字会使用默认的）得到
id_rsa # 私钥 id_rsa.pub # 公钥 重复上一个步骤，生成多个 私钥和公钥 github_id_rsa github_id_rsa.pub gitlab_id_rsa gitlab_id_rsa.pub 配置相应的域名对应的 SSH-KEY 本地目录 ~/.ssh/ 下，查阅有没有 config 文件, 不存在就新建 config 文件 Host github HostName github.com User UserName PreferredAuthentications publickey IdentityFile ~/.ssh/github_id_rsa Host gitlab HostName gitlab.com User UserName PreferredAuthentications publickey IdentityFile ~/.ssh/gitlab_id_rsa 将密钥添加进入 SSH-AGENT 中 ssh-add ~/.ssh/github_id_rsa ssh-add ~/.ssh/gitlab_id_rsa 查看密钥</description></item><item><title>使用githook统一codestyle</title><link>https://pinkhello.cc/posts/09-%E4%BD%BF%E7%94%A8githook%E7%BB%9F%E4%B8%80codestyle/</link><pubDate>Tue, 30 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/09-%E4%BD%BF%E7%94%A8githook%E7%BB%9F%E4%B8%80codestyle/</guid><description>gradle 优化 build 指定 -g cache 缓存 checkstyle 实践 基础镜像包含 checkstyle.xml 或者 放到远程其他可被拉取到的存储介质 ，防止项目成员改动 gitlab-ci beforeScript 标签执行命令 copy /checkstyle.xml 进入项目，(覆盖项目中存在的). gradle 编译的话 将 maven-publish.gradle repos.gradle checkstyle.gradle(checkstyle 插件配置 版本以及 configFile) 抽出放到公共的地方，防止项目团队成员改的. maven 的话，可以在公共的顶级继承 pom 里面指定变量checkstyle.config.location. mvn checkstyle -Dcheckstyle.config.location=checkstyle.xml git hook 实践 每个项目里面 .git/hooks 里面有很多的 hook 模板
客户端钩子包括:pre-commit、prepare-commit-msg、commit-msg、post-commit等，主要用于控制客户端git的提交工作流。
服务端钩子：pre-receive、post-receive、update，主要在服务端接收提交对象时、推送到服务器之前调用。
今天实践的是 客户端钩子，优化减少不符合规范或者低质量代码进入 gitflow 流程.
pre-commit 和 commit-msg 是今天的主角，pre-commit 执行与 git add 之后，在进行 git commit 之前进行的操作. 可以用来进行 code check code lint 等等, commit-msg 执行与 git commit 常用于补全 git commit message check msg 等等 当然还有其他骚操作的功能，可以通知，等等，做多种自动化</description></item><item><title>Gradle多模块项目模板化</title><link>https://pinkhello.cc/posts/08-gradle%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E6%A8%A1%E6%9D%BF%E5%8C%96/</link><pubDate>Mon, 29 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/08-gradle%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E6%A8%A1%E6%9D%BF%E5%8C%96/</guid><description>前言 Maven 冗余， Gradle 简单轻便 公司原有的 CI/CD 流程，借助 Maven 插件 build Docker Image,改为原生 Docker Runner 原始构建
1、多模块项目 project - app - src/main/[java|resources] | src/test/[java|resources] # classpath - Dockerfile # Dockerfile - build.gradle # APP 模块 gradle 配置 - sdk # SDK 模块 可有可无 - src/main/java - build.gradle # SDK 的 gradle 配置 - deploy # delpoy 项目 注意 checkstyle 相关配置在这里面 - checkstyle/** - **.yml - build.gradle # 项目顶级 gradle配置 - gradle - wrapper/** # gradle 配置信息 - check.</description></item><item><title>Fabric使用</title><link>https://pinkhello.cc/posts/07-fabric%E4%BD%BF%E7%94%A8/</link><pubDate>Sun, 28 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/07-fabric%E4%BD%BF%E7%94%A8/</guid><description>docker 加入systemctl环境并启动docker 快速安装docker
curl -sSL https://get.daocloud.io/docker | sh systemctl enable docker systemctl start docker docker-compose 安装 走外网或者 github 太慢,可以使用内部加速
curl -L &amp;#34;https://github.com/docker/compose/releases/download/X.XX.X/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-compose curl -L https://get.daocloud.io/docker/compose/releases/download/1.26.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose fabric 自动运维 python 虚拟环境安装 参考
创建一个独立的虚拟环境
cd 目标目录 virtualenv --no-site-packages venv 激活虚拟环境
source venv/bin/activate python pip 安装 Fabric pip install fabric3 python pip 导出依赖 pip freeze &amp;gt; requirements.txt 其他python pip 导入安装 pip install -r requirements.txt Fabric 文档 fabfiles 文档 # encoding=utf-8 from fabric.</description></item><item><title>高性能队列Disruptor</title><link>https://pinkhello.cc/posts/06-%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97disruptor/</link><pubDate>Mon, 15 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/06-%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97disruptor/</guid><description>背景 Disruptor 是 外汇交易公司LMAX开发的高性能队列、研发是为了解决内存队列延迟问题。 Disruptor 一般用于线程间的消息传递。 Disruptor GitHub 地址
Disruptor 介绍 理解 Disruptor 最好的方式，选择一个最接近熟悉的样本进行比较。在这个前提下，可以选择 Java 中的 BlockingQueue. 和队列相似，Disruptor 也是在同一个进程中不同的线程之间进行传递数据的（例如消息或者事件），同时 Disruptor 提供了一些将关键功能和队列分开的特性：
向消费者发送多播事件 消息者依赖关系图 预先为事件分配内存 可选的（无锁） Disruptor 核心概念 在我们理解Disruptor如何工作之前，了解下核心概念
Ring Buffer 环形数组设计，为了避免垃圾回收，采用的数组结构，从3.0开始，环形缓冲区主要存储和更新在Disruptor中移动的数据（事件） Sequence Disruptor 每个消费者(EventProcessor)维护一个 Sequence，并发的大多数代码都依赖 Sequence 值的改动，所以 Sequence 支持 AtomicLong 的大部分也行, 唯一不同的是 Sequence 包含额外的功能来阻止Sequence和其他值之间的伪共享(false sharing) Sequencer
Disruptor 核心逻辑, 两个实现: 单生产者和多生产者。他们实现了生产者与消费者之间的快速传递的并发算法。 Sequence Barrier 由 Sequencer 生成，包含此 Sequencer 发布的 Sequence 指针以及依赖的其他消费者的 Sequence。包含了消费者检查是否有可用的事件的代码。 Wait Strategy 消费者等待事件的策略，这个事件由生产者放入，决定了消费者怎么等待生产者将事件放入 Disruptor Event 生产者与消费者传递的事件，完全由用户定义 EventProcessor 处理事件的主要循环（main event loop），包含了一个 Sequeuece.</description></item><item><title>OAuth2.0 那点事</title><link>https://pinkhello.cc/posts/05-oauth2.0%E9%82%A3%E7%82%B9%E4%BA%8B/</link><pubDate>Wed, 10 Feb 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/05-oauth2.0%E9%82%A3%E7%82%B9%E4%BA%8B/</guid><description>OAuth2.0 是什么? OAuth2.0 Framework RFC 6749 [https://tools.ietf.org/html/rfc6749]
OAuth 就是一种授权机制，它介于客户端与资源所有者的授权层，为了分离不同的角色。 在资源所有者同意并向客户端颁发令牌后，客户端携带令牌可以访问部分或全部资源。
OAuth2.0 是OAuth 协议的一个版本，为2.0版本。有意思的是 2.0 与 1.0 并不兼容。
OAuth2.0 授权方式 获取授权的过程
授权码(authorization-code) 隐藏式(implicit) 密码(password) 客户端凭证(client credentials) 不管哪种方式，都需要在第三方应用申请令牌之前，需要在系统中申请身份唯一标识: 客户端ID Client ID 和 客户端秘钥 Client Secret. 这样能确保Token不被恶意使用。
授权重要的参数和指标:
response_type 响应类型: code(要求返回授权码),token(要求返回授权Token) client_id 客户端身份标识 client_secret 客户端秘钥 redirect_uri 重定向地址 scope 授权范围, read 只读权限, all 全部权限 grant_type 授权方式 authorization_code(授权码)、password(密码)、client_credentials (凭证)、refresh_token(更新令牌) state 应用程序传递的一个随机数，防止 CSRF 攻击 授权码 在访问第三方应用先申请一个授权码，然后再用授权码获取令牌.这种方式也是最常用的流程，安全性也是最高的，适用于有后端的Web应用。授权码通过前端传送，令牌存储在后端。所有的和资源服务器的交互都在服务端完成，避免了令牌的泄露。 授权码和令牌的在 浏览器和客户端WEB应用以及资源服务器的交互流程大致如下: 1.2.3.4 用户选择 Google 登陆 yelp.com 3.4 Yelp.com 请求用户授权 Google 权限 5.</description></item><item><title>如何构建一个简单的RPC调用</title><link>https://pinkhello.cc/posts/04-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84rpc%E8%B0%83%E7%94%A8/</link><pubDate>Wed, 13 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/04-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84rpc%E8%B0%83%E7%94%A8/</guid><description>1、什么叫RPC?
RPC构成
RPC Consumer RPC Provider ConfigServer 1、Provider 启动 ConfigServer 注册服务 2、Consumer 启动 ConfigServer 订阅服务， 3、发起调用 Consumer &amp;mdash;&amp;gt; Provider 4、响应调用 Consumer &amp;lt;&amp;mdash; Provider 2、什么是 Netty ? https://netty.io/
3、现有的开源的项目是否使用了 Netty ?
Dubbo Grpc Spark &amp;hellip;. 4、RPC Provider 启动
Netty Server 方式启动 Rpc 服务的注册 5、RPC Consumer 启动
Netty Client 方式启动 RPC 泛化调用、通过字节码基于反射来实现远程调度 Consumer 服务订阅 启动时建立长连接 6、从第四可以看出，多个 Provider 是由一个 NettyServer 提供的，通过 HandlerMap 映射找到对应的 Ioc Bean，完成服务调用
服务端 EventLoopGroup bossGroup = new NioEventLoopGroup(1); EventLoopGroup workerGroup = new NioEventLoopGroup(); try { ServerBootstrap b = new ServerBootstrap(); b.</description></item><item><title>String为什么设计成final</title><link>https://pinkhello.cc/posts/03-string%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E8%AE%A1%E6%88%90final/</link><pubDate>Tue, 12 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/03-string%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E8%AE%A1%E6%88%90final/</guid><description>String源码剖析 public final class String implements java.io.Serializable, Comparable&amp;lt;String&amp;gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; /** * Class String is special cased within the Serialization Stream Protocol. * * A String instance is written into an ObjectOutputStream according to * &amp;lt;a href=&amp;#34;{@docRoot}/.</description></item><item><title>关于final的思考</title><link>https://pinkhello.cc/posts/02-%E5%85%B3%E4%BA%8Efinal%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Mon, 11 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/02-%E5%85%B3%E4%BA%8Efinal%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>关于final的思考 final 是声明数据域最终的,不可以修改的，常见的 是类的 序列化ID String 类，其数据域都是 final 的 修改 final 修饰的属性 反射修改 final 修饰的数据域【非常成功的修改了】
public class Test { private final String name = &amp;#34;hello world&amp;#34;; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { Test test = new Test(); Field field = test.getClass().getDeclaredField(&amp;#34;name&amp;#34;); field.setAccessible(true); field.set(test,&amp;#34;HELLO, WORLD!&amp;#34;); System.out.println(field.get(test)); System.out.println(test.name); } } 输出 Hello, WORLD! hello world 第一个输出是因为说明运行成功，修改final修饰的对象的属性成功修改；
但是第二个输出，表明了我直接使用 name 的属性却还是输出端额原来的值.
反编译后的代码
public class Test { private final String name = &amp;#34;hello world&amp;#34;; public Test() { } public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { Test test = new Test(); Field field = test.</description></item><item><title>一致性哈希算法</title><link>https://pinkhello.cc/posts/01-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/</link><pubDate>Sun, 10 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/01-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/</guid><description>分布式均衡寻址算法 在分布式集群中，对机器的添加删除，或者机器故障后自动脱落集群的操作是分布式集群管理的基本功能。
在集群环境中，判断分布式寻址算法好坏的原则：
平衡性（Balance） 单调性（Monotonicity） 分散性（Spread） 负载（Load） Hash(Object)%N 集群N台机器，根据N取模，路由到对应的机器，但是缺点在于，对于机器的添加删除，已经缓存的数据都失效、严重违反单调性， 大量的缓存重建
假设0-3个节点、20个数据: 进行取模后分布: 扩容后: 当前只有4个数据能命中。命中率 4/20 = 20% ,命中率底下，并且有大量缓存需要重建
一致性Hash ( DHT ) 公共哈希函数和哈希环 Hash算法设计: 采取取模方式，按常用的 Hash 算法将对应的 Key 哈希到一个具有 2^32 次方的桶空间中，即 0 ~ (2^32)-1 的数字空间。想想一下，将数字首位相连，组成一个闭合的环形。 对象(Object)映射到哈希环 把对象映射到 0-2^32-1 空间里，假设有4个对象 object1-4 ，映射进hash环状 缓存(Cache)映射到哈希环 下面将 Cache 映射进 Hash 空间，假设现在有三个cache：
基本思想就是 Object 和 Cache 都映射到同一 Hash 数值空间中，并且使用相同的 Hash算法，可以使用 Cache 的 IP地址或者其他因子）
对象（Object映射到缓存(Cache)节点 每个 Cache 的 Key 顺时针，找到第一个 Cache 节点就是存储位置: 移除一个缓存节点 移除一个 CacheB 节点, 这时候 key4 无法找寻到 Cache，key4将继续使用一致性Hash算法算出最新的 CacheC, 以后存储与读取都在 CacheC 上。 移除节点后的影响范围在该节点逆时针计算到遇到的第一个cache节点之间的数据节点。</description></item><item><title>Threadlocal 魔法</title><link>https://pinkhello.cc/posts/00-threadlocal-%E9%AD%94%E6%B3%95/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-threadlocal-%E9%AD%94%E6%B3%95/</guid><description>ThreadLocal 详解 前言 对于 ThreadLocal 的使用，并不难，这次主要讲述 ThreadLocal 的实现方式以及原理
ThreadLocal 是什么 ThreadLocal 为解决多线程并发问题提供的一种新的思路。
当使用 ThreadLocal 维护变量的时候，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每个线程都可以独立修改自己的副本，而不会修改到其他人的变量副本。
从线程角度看，Local 即本地意思，目标变量就像是线程的本地变量。
原理 ThreadLocal 是连接 Thread 与 ThreadLocalMap 粘合剂，是用来处理 Thread 的 ThreadLocalMap 属性， 包括 initialValue() 变量，set 对应的变量，get 对应的变量。
ThreadLocalMap 用来存储数据，采用类似HashMap的机制，存储了以ThreadLocal为Key，目标数据为Value的Entry键值对数组结构。
Thread 有个 ThreadLocalMap 的属性，存储的数据存放在此处。
Thread、ThreadLocal、 ThreadLocalMap的关系 ThreadLocalMap 是 ThreadLocal 的内部类，有 ThreadLocal创建，Thread有 ThreadLocal.ThreadLocalMap 类型的属性，源码如下
Thread public class Thread implements Runnable { /* * ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class.</description></item><item><title>数据结构与算法 01 优先队列</title><link>https://pinkhello.cc/posts/00-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-01-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-01-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/</guid><description>为什么需要优先队列 队列是一种先进先出的数据结构，所有元素优先级一样，完全遵守先进先出的规则。但是往往现实情况下，这种公平需要被打破。它是一个动态变化的过程，可能有一些需要优先，一些需要降低优先级。且这些数据是一个动态变化的过程，所以需要维系这个优先级队列。
优先队列的实现方式 数组实现 链表</description></item><item><title>算法 Bitmap</title><link>https://pinkhello.cc/posts/00-%E7%AE%97%E6%B3%95-bitmap/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-%E7%AE%97%E6%B3%95-bitmap/</guid><description>bitmap 原理 bitmap字面为位图映射, 原理是使用一个 bit 标记某个元素对应的 value，而 key 即该元素。因为只有一个 bit 来存储一个数据, 因而可以大大的节省空间。
数值映射: 假如对 0-31 个内的3个元素（10, 17, 28）进行排序,可以采用 BitMap 方法, 如下图, 对应的包含的位置将对应的值从 0 变更为 1 假如需要进行排序和检索，只需要依次遍历这个数据结构，碰到 1 的情况，数据存在
字符串映射: 字符串也可映射，只不过需要经过一个Hash步骤,通过映射关系可以判断字符串是否存在。但是因为 Hash是将不确定长度的值变更为确定大小的值,存在Hash冲突性，所以一般要最大化的判断一个字符串是否真的存在，可以将这个字符串经过不同的Hash函数映射不同的位置。
bitmap 的 建立、查找、添加、删除、判断 原理 建立 Bitmap 的创建可以使用 byte 数组， 1 byte = 8 bit (也可使用 int 数组, 1 int = 32 bit, long 数组, 1 long = 64 bit) 也就是说到最后的数据的大小建立只需要创建 数组长度为 int[ 1 + N/32 ] byte[ 1 + N/8 ] long[ 1 + N/64 ] 即可存储，N表示要存储的最大的值。</description></item></channel></rss>