<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tech on pinkhello</title><link>https://pinkhello.cc/categories/tech/</link><description>Recent content in Tech on pinkhello</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>pinkhello</copyright><lastBuildDate>Mon, 28 Mar 2022 10:23:58 +0800</lastBuildDate><atom:link href="https://pinkhello.cc/categories/tech/index.xml" rel="self" type="application/rss+xml"/><item><title>Gitlab CI CD</title><link>https://pinkhello.cc/posts/cicd/</link><pubDate>Mon, 28 Mar 2022 10:23:58 +0800</pubDate><guid>https://pinkhello.cc/posts/cicd/</guid><description>基本概念 CI，Continuous Integration 持续集成， 即在代码构建过程中持续的对代码进行集成、构建、自动化测试。 CD，Continuous Deployment 持续交付，即在代码构建完毕后，可以方便的进行部署上线，快速迭代交付产品。 Gitlab CI/CD 先看一张图，介绍了 Gitlab CI/CD 的工作流程
上图是一个通用的开发流程，从代码实现开始、提交，通过代码的改变触发 CI/CD pipeline，后续通过 CR 和 Approve，代码才合并进入分支/
更丰富的操作:
CI/CD pipelines pipelines 包含两个核心的，它是 CI/CD 顶级组件。
Jobs 定义了要做什么。(eg：compile job or test job) Stages 定义了如何做。 如果一个 Stage 中的所有的 Job 都成功，则 pipeline 进入下一个 Stage。 如果一个 Stage 中任何 Job 失败，一般不会到下一个 Stage，并且 pipeline 会提前结束。
典型的一个 pipeline 的操作:
build stage， 使用一个 job 去 compile 代码 test stage，使用 多个 job 去跑 ut test staing stage，使用 分布部署阶段 production stage，启动 job 生成部署阶段 Jobs pipeline 的配置从 job 开始了，job 是 .</description></item><item><title>Golang for Range 探究</title><link>https://pinkhello.cc/posts/golang-for-range-%E6%8E%A2%E7%A9%B6/</link><pubDate>Sun, 27 Mar 2022 14:20:28 +0800</pubDate><guid>https://pinkhello.cc/posts/golang-for-range-%E6%8E%A2%E7%A9%B6/</guid><description>前言 今天在用 Golang 写 TCP 长链接的时候，在做测试的时候 for range 有在结构包的时候有性能问题。所以就细化探究了一下。
复现 假设有一个 Packet 类型([1024]byte)的数据包,我们从网络中获取到一个 [1024]Packet 的数据包，我们进行遍历
type Packet [1024]byte func BenchmarkForStruct_test(b *testing.B) { var items [1024]Packet var result Packet for i := 0; i &amp;lt; b.N; i++ { for k := 0; k &amp;lt; len(items); k++ { result = items[k] } } _ = result } func BenchmarkRangeStruct_test(b *testing.B) { var items [1024]Packet var result Packet for i := 0; i &amp;lt; b.</description></item><item><title>细谈线程池设计与监控</title><link>https://pinkhello.cc/posts/47-%E7%BB%86%E8%B0%88%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1%E4%B8%8E%E7%9B%91%E6%8E%A7/</link><pubDate>Sat, 05 Mar 2022 22:05:23 +0800</pubDate><guid>https://pinkhello.cc/posts/47-%E7%BB%86%E8%B0%88%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AE%BE%E8%AE%A1%E4%B8%8E%E7%9B%91%E6%8E%A7/</guid><description>前言 万年八股文，在工作中肯定多多少少都使用了线程池，但有没有真正的去了解线程池的整个运行机制和核心关键点。今天分享一下。
线程池简介 先说 线程(Thread) , 在 Wiki 的解释是操作系统能进行运算调度的最小单元。包含在进程中，是进程实际运作的单元。 OK，先记住这一点，我们说 线程池(Thread Pool) ，顾名思义，基于池化思想的管理线程的手段（工具）。既然叫线程池了，可以 知道肯定是多线程的服务环境下。
我们都知道Server的资源是有限的，不可能无止境的扩张（从成本、运维上考虑）。线程过分的开辟的话，会导致整体服务器资源吃紧（线程带来的开销、创建、调度等等）。 池化的思想，线程池维护着多个线程，等待调度分配并发任务去执行。一方面避免有任务就创建线程带来的额外的开销，另一方面也是保护服务器产生过多的线程数量。
线程池解决什么问题 线程池解决的核心问题是资源管理问题，在并发环境下，系统和服务并不知道在什么时候需要多少资源。而 池化(Pooling) 思想，就是为了将资源统一管理的情况下，减小风险的思想。
Wiki上 ThreadPool图
P.S : 池化思想举例
内存池(Memory Pool) 对象池(Object Pool) 即实例池 连接池(Connection Pool) 线程池核心设计 在看线程池的设计之前，先了解一下线程池的构造参数
corePoolSize – the number of threads to keep in the pool, even if they are idle, unless allowCoreThreadTimeOut is set maximumPoolSize – the maximum number of threads to allow in the pool keepAliveTime – when the number of threads is greater than the core, this is the maximum time that excess idle threads will wait for new tasks before terminating.</description></item><item><title>LRUCache 的实现</title><link>https://pinkhello.cc/posts/46-lrucache-%E5%AE%9E%E7%8E%B0/</link><pubDate>Thu, 10 Feb 2022 18:59:50 +0800</pubDate><guid>https://pinkhello.cc/posts/46-lrucache-%E5%AE%9E%E7%8E%B0/</guid><description>文引 要实现 LRU Cache，我们需要了解 LRU Cache 的运行原理。
LRU Cache LRU (Least Recently Used) Cache 是一种缓存淘汰算法，最少最近使用的Cache。也就是说，在淘汰的时候，淘汰的是最长时间未使用（或者使用最少）的数据
如下图，容量是为 3 的缓存:
一开始，Cache 为空,添加一个 12 元素进入，后面依次放入了 11 和 10。这时候将缓存填满了。 将放入 9 的时候，因为缓存满了，需要移除最长时间未使用的数据。所以 12 被移除，9 被加入。 P.S.: 这边未考虑使用操作，其实也可以想像的到，如果要是使用了，就将使用的元素从缓存移除，重新加入到缓存中
经过上述了解，已经直到了 LRU 的运行机制，这样我们总结特点：
Cache 大小固定有限 Cache Full 后，后续操作都需要执行 LRU 操作 Cache 操作支持并发 Cache 的 添加、排序、获取 尽量都是 O(1) 操作 Structure of LRU Cache 要考虑设计 LRU Cache，我们需要根据其特点来设计：
特性1: LRU Cache 是一个 Queue，如果 Queue 里面的某个元素被访问了，它应该被迁移到淘汰的末位。 特性2:LRU Cache 要有固定容量（内存有限）, 当新加一个元素的时候，是添加到 Queue Head；当发送淘汰的时候，淘汰的是 Queue Tail。（FIFO） 特性3: 查找命中缓存的数据，必须在 最少的固定的时间内 完成。也就是我们我们必须尽量保证时间复杂度 O(1) 特性4: 移除最近最少使用的元素，也必须在 最少的固定的时间内 完成。同上 O(1) 要满足 特性1 和 特性2 还是很简单的,要实现一个 Queue 我们可以使用数据结构 固定大小的Linked List 或 固定大小的Array List 做实现。 但是在兼顾 特性3 的时候，可以想象到要尽量 添加、命中 都是 O(1) 操作，在 Queue 中基本不可能。但是我们可以预判到 HashMap 可以最大化的解决这个问题， 因为HashMap最理想的情况下达到 命中缓存为 O(1) 操作，到此，我们发现 LRU Cache 的设计应该是 HashMap + List； 下面再去考虑 特性4，在 Cache 满的情况下，考虑每次添加新的元素都要执行LRU策略，这时候可预见的对于这个 List的插入``删除比查询多， 肯定选择是 Linked List，且是 DoublyLinkedList 不是 SingleLinkedList (出于操作O(1)考虑)</description></item><item><title>如何去实现RingBuffer</title><link>https://pinkhello.cc/posts/45-%E5%A6%82%E4%BD%95%E5%8E%BB%E5%AE%9E%E7%8E%B0ringbuffer/</link><pubDate>Thu, 10 Feb 2022 14:39:59 +0800</pubDate><guid>https://pinkhello.cc/posts/45-%E5%A6%82%E4%BD%95%E5%8E%BB%E5%AE%9E%E7%8E%B0ringbuffer/</guid><description>文引 RingBuffer, 名如其意: 环形缓存区/环形队列，不同于一般的队列，特征是首尾相接。
RingBuffer 如何工作的 RingBuffer 是一个有界的循环的数据结构，主要用于多线程下进行的数据缓存。在持续的写入数据的时候，到达末尾的时候链接到头，效果上达成一个环状。
实现它的方式 它是有界的数组实现，如图。
而且我们还要关注到 reader指针 、 writer指针、 头尾相连
reader pointer 下一个可读元素 writer pointer 下一个可插入的元素 slot 数组头 和 数组尾 相互链接 讨论 ringbuffer 的运行方式 ringbuffer 的关键参数, read seq &amp;amp; write seq
reader pointer seq =&amp;gt; 从 0 开始，随着读取消耗一个元素 +1 writer pointer seq =&amp;gt; 从 -1 开始，插入一个元素时候 +1 可以看出两种 Seq 对 容量进行 Mod 操作可以将 seq 映射到 ringbuffer 的 index 上.
array_index = seq % capacity 基于上面的思想，我们看 ringbuffer 的核心操作</description></item><item><title>Golang的池化设计</title><link>https://pinkhello.cc/posts/43-golang%E7%9A%84%E6%B1%A0%E5%8C%96%E8%AE%BE%E8%AE%A1/</link><pubDate>Wed, 02 Feb 2022 02:52:28 +0800</pubDate><guid>https://pinkhello.cc/posts/43-golang%E7%9A%84%E6%B1%A0%E5%8C%96%E8%AE%BE%E8%AE%A1/</guid><description>Why Pool？ 先埋坑: Go 有没有池化的必要? 为什么要池化? Go 自从出生以来都被我们冠以&amp;quot;高并发&amp;quot;的Tag, 回头细想和深究一下，你会发现其实都是由 goroutine 实现的 , 我们都知道 与 Thread 相比， 创建 Goroutine 的代价非常小。从程序表现上，也像极了Thread, 每个 Program 至少包含一个 Goroutine(至少一个**Main Goroutine**) 所有的其他 Goroutines 都依附于 Main Goroutine（如果 Main Goroutine Terminated, 其他 Goroutines 也会 Terminated）, Goroutine 总是工作在后台。
多线程``多进程是为了提高系统的并发能力，当前系统下，这边就需要系统调度，一个线程可以拆分为 &amp;quot;用户态&amp;quot;线程 和 &amp;quot;内核态&amp;quot;线程，这两者需要进行绑定，我们一般称 内核态线程 为线程，用户态线程为协程
从上面可以看出 协程 和 线程 有映射关系，这样就来了 M:N 关系(为什么不是 1:1, N:1 ? )
Goroutine 优点:
相比 Thread 代价更小 Goroutine 其在 stack 的大小可以根据程序要求进行变化，但是 Thread 是固定的 Goroutine 使用 Channel 进行通信，Channel 用于 Goroutines 访问共享内存的时候防止竞争的。 从程序上一个线程可能拥有许多的 Goroutines 关联，如果这些关联的 有一个 Goroutine 去阻塞了线程，那么剩余的 Goroutine 将分配给其他的 OS Thread，且这种切换操作是对开发者屏蔽的。 Goroutine 调度器 回头看将 Goroutine 和 协程 有区别的。调度</description></item><item><title>MVCC在不同的数据库之中的应用</title><link>https://pinkhello.cc/posts/40-mvcc%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</link><pubDate>Thu, 02 Dec 2021 11:06:28 +0800</pubDate><guid>https://pinkhello.cc/posts/40-mvcc%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B9%8B%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/</guid><description>MVCC 高并发情况, 应对数据冲突问题，一般两种方式:
见招拆招，避免冲突，使用 悲观锁 确保同时刻只能一个线程对数据进行修改。（Read/Write Locks、Two-Phase Locking) 开门迎客，允许冲突，但发生冲突的时候，要有能力检测到。（乐观锁）先认为不会发生冲突、除非检测到当前确实冲突了。（Logical Clock、MVCC ： Multi-Version Concurrent Control ） MVCC
核心思想: 一个数据多个历史版本，从而解决事务管理中数据隔离的问题。
版本 &amp;mdash;&amp;gt; 一般选择时间戳 或者 事务ID 标识
在处理写请求的时候， MVCC 是为数据添加一个新版本的数据；在读取数据的时候，先确定读取的版本，根据版本找到对应的数据。
https://dbdb.io/browse?concurrency-control=multi-version-concurrency-control-mvcc&amp;amp;q=
MYSQL中 MVCC innodb-multi-versioning
InnoDB 内部为每一行记录添加 三个隐藏的列，
DB_TRX_ID 创建记录/最后更新记录的事务ID,(删除也被视作更新，设置行的特殊位标记位删除) DB_ROLL_PTR 回滚指针，执行这条记录的上一个版本(rollback segment ： undo log，记录了更新前重建该记录的所需的信息) DB_ROW_ID 隐藏自增ID, 如果没有主键，就会以DB_ROW_ID 产生聚簇索引，随着插入新行，自增的， rollback segment 分为两种
inert undo log ，仅在事务回滚的时候需要，事务提交后立即丢弃 update undo log ，用于一致性读取，只有在 没有事务存在，且 头信息里面还有一个 delete_flag (bit) 标记为该记录是否删除
[待丰富]&amp;hellip;..
ETCD 中 MVCC mvcc</description></item><item><title>探秘负载均衡</title><link>https://pinkhello.cc/posts/36-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Fri, 01 Oct 2021 10:04:00 +0800</pubDate><guid>https://pinkhello.cc/posts/36-%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>前言 今天是国庆的第3天，在此恭祝小哥哥和小姐姐们⛱️节日快乐！ 微信朋友圈🀄️全部是 旅途ING。
我的国庆：
第一天: 带🪆 第二天: 被启东坑了一回 第三天: 🈚️孩一身轻 &amp;hellip;&amp;hellip; 对于我这种社交尴尬者，⛱️度假最好的方式还是学习。
今日带来了 自己对于负载均衡(load balance)的学习。
度假最好的方式是学习 GRPC在负载均衡的设计: Load-balancing.md
为什么要负载均衡 在如今的互联网领域，对于数据的大爆发，高频的请求，对于提供服务的企业来说是不小的IT资源开销。为了解决资源中的负载分配，并且使得资源的利用率达到最大。出现了负载均衡，主要为了解决 高并发 和 高可用。
负载均衡的实现方式有：软件 和 硬件。这次我学习的主是软件方式。
负载均衡的算法 轮询( Round Robin ) &amp;amp; 加权轮询( Weight Round Robin ) : 随机 &amp;amp; 加权随机 最少连接( Last Connections ) : 通过将流量引导到第一个可用服务器然后将该服务器移动到队列底部来轮换服务器，当服务器具有相同的规格并且没有很多持久连接时最有用。 哈希( Hash ) eg: ip hash 一致性哈希( Consistent Hash ) eg: request_url 一致性哈希 最少响应时间 : 将流量定向到活动连接最少且平均响应时间最短的服务器。 常用的负载均衡手段 Load-balancing.md 已经介绍了以下几种</description></item><item><title>短链接的思考与技术实现</title><link>https://pinkhello.cc/posts/35-%E7%9F%AD%E9%93%BE%E6%8E%A5%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0/</link><pubDate>Sun, 05 Sep 2021 00:19:13 +0800</pubDate><guid>https://pinkhello.cc/posts/35-%E7%9F%AD%E9%93%BE%E6%8E%A5%E7%9A%84%E6%80%9D%E8%80%83%E4%B8%8E%E6%8A%80%E6%9C%AF%E5%AE%9E%E7%8E%B0/</guid><description>前提 又到了一月一度的清理消息的时候了,开始清理短信,忽然见到一个幸福里的通知短信，那是在短信到达之前，本🐶打肿脸充胖子在幸福里小程序里面咨询上海的一套二手房的价格的，后来没关注， 这个短链接 https://m.xflapp.com/s/vdwecR 引起了本🐶的兴趣.
我发问：如何去实现一个短链接服务?
短链接的原理 短链接的核心是什么?
构建的短链接和长链接(我们真实的访问地址)的唯一关联映射、也就是映射标识生成算法。
从这个定义也能得到短链接服务的几个特点:
1⃣️具有高性能 2⃣️是排列组合数量量要足够大 3⃣️要具有不易破解、且破解难度极大 拆解短链接 👆的短链接 https://m.xflapp.com/s/vdwecR 我掏出了我的神器 Chrome 打开了一下，直接跳转到了http://m.haoduofangs.com/f100/activity/client/tt_im?app_id=13&amp;amp;customer_user_id=62800686641&amp;amp;realtor_id=3781951248937422. 好了仔细看一下请求和响应,响应的302,响应头中带了需要跳转的地址.
综上我示意图:
可以看出来,构建唯一映射关系是将原有的固定长链接生成一个或多个短链接,1:N关系。生成的短链接必须满足:
不能轻易的被猜出来(破解),被恶意遍历。 [一定时间内]不能重复(一个短链接在一定时间内只能对应一个长链接、这个一定时间可以是永久) 长度尽量短 在短信运营商会限制短信的长度、如果太长、留给运营人员或者客户的的字符长度会有🚫，这不是给同事或客户挖坑么。 链接变二维码，二维码的点位会非常密集，不利于客户端识别和传输 长度太长，从个人情感里面也比较繁琐这样的长链接，不易记录和输入 回到这个唯一映射关系上来，就是需要对这个长链接做 Hash，和大多数哈希算法一致,生成算法需要就是要具备唯一性和较低的碰撞率的特点,而在短链接场景下，又决定了它还要具备短线精悍并且易于传输的特点。
短链接压缩生成算法特点:
短小精悍、易于传输 高度唯一性 低碰撞率 短链接生成算法 从上图可以看出，协议、域名均为不可变部分，能定制的只有压缩码(哈希码), 那么作为程序🐶能玩的也只有这部分了。
回到幸福里通知短信的短链接, 它抛掉协议和域名两部分,还剩下s/vdwecR, 可以看出Path其实是两层,第一层是s,第二层是vdwecR， 可以抽象为{pathLevel1}/{pathLevel2},{pathLevel1}为了简短使用了单个字母(可以区分大小写)或者数字, {pathLevel2} 为了满足低碰撞率和唯一性，使用了6位区分大小写字母和数字的组合体。 那么像这个组合体的最大组合数量是多少呢？{pathLevel1} 假设只有一位, {pathLevel2} 有6位,从当前的例子来看，不考虑取值为数字的可能，那么它的组合数 每个取值都是(26个大写字母 + 26个小写字母) ，也就是 52 ^ 7 = 1,028,071,702,528 ,已达万亿级别数量级。</description></item><item><title>Aws上mongo备份进入S3</title><link>https://pinkhello.cc/posts/34-aws%E4%B8%8Amongo%E5%A4%87%E4%BB%BD%E8%BF%9B%E5%85%A5s3/</link><pubDate>Fri, 27 Aug 2021 17:58:15 +0800</pubDate><guid>https://pinkhello.cc/posts/34-aws%E4%B8%8Amongo%E5%A4%87%E4%BB%BD%E8%BF%9B%E5%85%A5s3/</guid><description>机器准备 开启新机器, 注意需要备份的数据量。选择磁盘或者数据盘大小 老机器上, 磁盘在线需要扩容的话注意执行扩容，让设备分区和文件系统都识别 云台上选择需要扩容的磁盘扩大到需要的大小（当前机器上还无法直接使用） 进入机器控制台 # df -lh # lsblk # growpart 磁盘设备名称 1 # resize2fs 磁盘文件系统 # df -lh 备份脚本准备 backup.sh 备份脚本 #!/bin/bash # 公共 function 日志日期格式 get_date () { date +[%Y-%m-%d\ %H:%M:%S] } ARCHIVE_OUT=$BACKUP_FILENAME_PREFIX-$(date +$BACKUP_FILENAME_DATE_FORMAT).tgz echo &amp;#34;ARCHIVE_OUT=$ARCHIVE_OUT&amp;#34; # 生成POST请求数据 generate_post_data () { cat &amp;lt;&amp;lt;EOF { &amp;#34;group&amp;#34;:&amp;#34;alert-XXX&amp;#34;, &amp;#34;project&amp;#34;:&amp;#34;$S3_PATH$ARCHIVE_OUT&amp;#34;, &amp;#34;alert_message&amp;#34;:&amp;#34;备份mongo2s3错误&amp;#34; } EOF } # Script echo &amp;#34;$(get_date)Mongo backup started&amp;#34; echo &amp;#34;$(get_date)[Step 1/4] Running mongodump: mongodump --forceTableScan -h $MONGO_HOST$MONGO_DB_ARG-u $MONGO_USERNAME-p $MONGO_PASSWORD--authenticationDatabase admin&amp;#34; # mongodump --quiet -h $MONGO_HOST:$MONGO_PORT $MONGO_DB_ARG -u $MONGO_USERNAME -p $MONGO_PASSWORD --authenticationDatabase admin mongodump --forceTableScan -h $MONGO_HOST $MONGO_DB_ARG -u $MONGO_USERNAME -p $MONGO_PASSWORD --authenticationDatabase admin echo &amp;#34;$(get_date)[Step 2/4] check dump directory: curl feishu .</description></item><item><title>RocketMQ源码阅读 Consumer</title><link>https://pinkhello.cc/posts/28-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-consumer/</link><pubDate>Sat, 31 Jul 2021 15:10:43 +0800</pubDate><guid>https://pinkhello.cc/posts/28-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-consumer/</guid><description>RocketMQ Consumer 分为两类,
DefaultMQPullConsumer 标记为弃用 DefaultMQPushConsumer 我们这次只分析 DefaultMQPushConsumer
当前先看个 PushConsumer 代码
public class PushConsumer { public static void main(String[] args) throws InterruptedException, MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(&amp;#34;CID_JODIE_1&amp;#34;); /** * 设置订阅的 可以对指定消息进行过滤，例如：&amp;#34;TopicTest&amp;#34;,&amp;#34;tagl||tag2||tag3&amp;#34;,*或null表示topic所有消息 */ consumer.subscribe(&amp;#34;TopicTest&amp;#34;, &amp;#34;*&amp;#34;); /** * CONSUME_FROM_LAST_OFFSET 第一次启动从队列最后位置消费，后续再启动接着上次消费的进度开始消费 * CONSUME_FROM_FIRST_OFFSET 第一次启动从队列初始位置消费，后续再启动接着上次消费的进度开始消费 * CONSUME_FROM_TIMESTAMP 第一次启动从指定时间点位置消费，后续再启动接着上次消费的进度开始消费 */ consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); //wrong time format 2017_0422_221800 consumer.setConsumeTimestamp(&amp;#34;20181109221800&amp;#34;); /** * 注册并发消费（消费监听） */ consumer.</description></item><item><title>RocketMQ源码阅读 Producer</title><link>https://pinkhello.cc/posts/27-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-producer/</link><pubDate>Fri, 23 Jul 2021 15:10:37 +0800</pubDate><guid>https://pinkhello.cc/posts/27-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-producer/</guid><description>在消息发送的时候, 我们先看 Producer 的启动代码
public class Producer { public static void main(String[] args) throws MQClientException, InterruptedException { DefaultMQProducer producer = new DefaultMQProducer(&amp;#34;ProducerGroupName&amp;#34;); producer.start(); for (int i = 0; i &amp;lt; 128; i++){ try { Message msg = new Message(&amp;#34;TopicTest&amp;#34;, &amp;#34;TagA&amp;#34;, &amp;#34;OrderID188&amp;#34;, &amp;#34;Hello world&amp;#34;.getBytes(RemotingHelper.DEFAULT_CHARSET)); SendResult sendResult = producer.send(msg); System.out.printf(&amp;#34;%s%n&amp;#34;, sendResult); } catch (Exception e) { e.printStackTrace(); } } producer.</description></item><item><title>RocketMQ源码阅读 NameServer</title><link>https://pinkhello.cc/posts/26-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nameserver/</link><pubDate>Tue, 25 May 2021 19:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/26-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-nameserver/</guid><description>NameServer 角色 NameSever 在 RocketMQ 起到重要的角色，承担着路由管理、服务注册、服务发现等核心功能。
接收 Broker 的请求注册 Broker 路由信息 接收 Client 请求根据某个 topic 获取所有到 broker 的路由信息 NameSrv 核心类 NamesrvStartup public class NamesrvStartup { //... public static NamesrvController main0(String[] args) { try { NamesrvController controller = createNamesrvController(args); //启动 NamesrvController start(controller); String tip = &amp;#34;The Name Server boot success. serializeType=&amp;#34; + RemotingCommand.getSerializeTypeConfigInThisServer(); log.info(tip); System.out.printf(&amp;#34;%s%n&amp;#34;, tip); return controller; } catch (Throwable e) { e.</description></item><item><title>RocketMQ源码阅读 通信组件</title><link>https://pinkhello.cc/posts/25-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E9%80%9A%E4%BF%A1%E7%BB%84%E4%BB%B6/</link><pubDate>Sat, 22 May 2021 16:09:45 +0800</pubDate><guid>https://pinkhello.cc/posts/25-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E9%80%9A%E4%BF%A1%E7%BB%84%E4%BB%B6/</guid><description>RocketMQ 核心基石 前面已经介绍了 RocketMQ 的基本的概念和组件。今天我们开启真正的源码的阅读诗篇, RocketMQ 消息系各个组件 Producer、Consumer、Broker、NameSrv 通通离不开交互，那是使用的什么交互的呢。答案是TCP长链接。 而 RocketMQ 开源代码内部，对通信相关的进行了一次封装，都在 rocketmq-remoting 模块下，这个模块被其他 client、broker、namesrv 应用。
直接先说 remoting 的实现是基于 netty 做了封装、启动了服务端和客户端，支持三种消息的发送方式:
同步发送 单向发送 (不需要关注响应) 异步发送 下图为异步通信流程 remoting 包下的核心接口体系 接口 RemotingService public interface RemotingService { // 开启 void start(); // 关闭 void shutdown(); // 注册 RPCHook void registerRPCHook(RPCHook rpcHook); } 接口 RemotingServer public interface RemotingServer extends RemotingService { // 注册请求类型的处理器 【common 模块的 org.</description></item><item><title>RocketMQ源码阅读 开篇</title><link>https://pinkhello.cc/posts/24-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E5%BC%80%E7%AF%87/</link><pubDate>Thu, 20 May 2021 10:02:20 +0800</pubDate><guid>https://pinkhello.cc/posts/24-rocketmq%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB-%E5%BC%80%E7%AF%87/</guid><description>RocketMQ 是什么? RocketMQ 是 Alibaba 捐赠给 Apache 的一款分布式、队列模型的开源消息中间件。
Github https://github.com/apache/rocketmq 从官网也能看出它的一些特性:
低延迟 高可用 万亿级的消息支持 &amp;hellip; RocketMQ 基本概念 RocketMQ 是由 Producer、Broker、Consumer 三部分组成, Producer 负责生产 Message, Consumer 负责消费 Message, Broker 负责存储 Message。 每个 Broker 可以存储多个 Topic 的消息, 每个 Topic 的消息也可以分片存储在不同的 Broker 上。 Message Queue 用于存储消息的物理地址，每个 Topic 的消息地址存储于对歌 Message Queue 中。 Consumer Group 由多个 Consumer 实例组成。
Producer 负责生产消息，同步发送、异步发送、顺序发送、单向发送。同步和异步需要 Broker 确认信息，单向发送不需要。
Consumer 负责消费消息，一般异步消费。一个消费者会从 Broker 拉取消息。（拉取式消费、推动式消费）
Broker Server 负责存储、转发消息。 接收 Producer 发送来的消息并存储、同时为 Consumer 拉取请求做准备。当然也存储这消息相关的元数据（消费组、消费进度偏移、主题、队列消息等）</description></item><item><title>回望K8S 持久化存储</title><link>https://pinkhello.cc/posts/22-%E5%9B%9E%E6%9C%9Bk8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</link><pubDate>Tue, 18 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/22-%E5%9B%9E%E6%9C%9Bk8s-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/</guid><description>PV、PVC、StorageClass 说的啥？ PV: 持久化存储数据卷，这个 API 主要定义的是一个持久化存储在宿主机上的一个目录。一般由运维人员进行定义，比如定义一个 NFS 类型的 PV
apiVersion: v1 kind: PersistentVolume metadata: name: nfs spec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.5 path: &amp;#34;/&amp;#34; PVC: POD 所希望使用的持久化存储的属性. 比如 Volume 的存储大小、可读写权限等. PVC 一般由开发人员创建、或者由 PVC模板的方式成为StatefulSet的一部分，由StatefulSet控制器负责创建带编号的PVC.
# 创建一个 1 GB 大小的PVC --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfs spec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi .</description></item><item><title>回望K8S 容器编排与Kubernetes作业管理</title><link>https://pinkhello.cc/posts/20-%E5%9B%9E%E6%9C%9Bk8s-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E4%B8%8Ekubernetes%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86/</link><pubDate>Mon, 17 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/20-%E5%9B%9E%E6%9C%9Bk8s-%E5%AE%B9%E5%99%A8%E7%BC%96%E6%8E%92%E4%B8%8Ekubernetes%E4%BD%9C%E4%B8%9A%E7%AE%A1%E7%90%86/</guid><description>Pod pod 是 Kubernetes 项目的最小的 API 对象，原子调度单位.
假设 &amp;ldquo;容器的本质是进程&amp;rdquo;，容器镜像就是 exe 安装包, kubernetes 是操作系统
Pod 最重要的一个事实是一个逻辑概念。它对于 Kubernetes 最核心的意义是 容器设计模式。Kubernetes 真正处理的还是宿主机上操作系统上的 Linux 容器的 Namespace 和 Cgroups，而不是一个所谓的 Pod 边界和隔离环境。
Pod 其实是一组共享了某些资源的容器。Pod 里面所有的容器，共享的同一个 Network Namespace，并且可以声明共享同一个 Volume.
Kubernetes 项目内部，Pod 实现需要使用一个中间容器，这个容器叫做 Infra 容器，在 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。
sidecar 典型的例子：WAR 包和 Web 服务器 POD后，将 WAR 包和 Tomcat 分别做成镜像，可以把他们容器结合在一起
--- apiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: # 启动后做了一件事 把应用的WAR包拷贝到 /app目录中，后退出 initContainers: - image: sample-war:v2 name: war command: [&amp;#34;cp&amp;#34;, &amp;#34;/sample.</description></item><item><title>回望K8S Kubernetes拼图</title><link>https://pinkhello.cc/posts/19-%E5%9B%9E%E6%9C%9Bk8s-kubernetes%E6%8B%BC%E5%9B%BE/</link><pubDate>Sun, 16 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/19-%E5%9B%9E%E6%9C%9Bk8s-kubernetes%E6%8B%BC%E5%9B%BE/</guid><description>kubernetes 安装 all 节点安装 Docker 和 Kubeadm 所有节点 root 用户下操作
&amp;gt; curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &amp;gt; cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF &amp;gt; apt-get update # 这一步安装的时候 kubeadm 和 kubelet、kubectl、kubernetes-cni 都会自动安装完毕 &amp;gt; apt-get install -y docker.io kubeadm 提示：如果 apt.kubernetes.io 因为网络问题访问不到，可以换成中科大的 Ubuntu 镜像源 deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main。
部署 Kubernetes Master 声明一个 kubeadm.yaml
--- apiVersion: kubeadm.k8s.io/v1alpha1 kind: MasterConfiguration controllerManagerExtraArgs: # 配置了自定义自动水平扩展 horizontal-pod-autoscaler-use-rest-clients: &amp;#34;true&amp;#34; horizontal-pod-autoscaler-sync-period: &amp;#34;10s&amp;#34; node-monitor-grace-period: &amp;#34;10s&amp;#34; apiServerExtraArgs: runtime-config: &amp;#34;api/all=true&amp;#34; # kubeadm 部署的 kubernetes 的版本 kubernetesVersion: &amp;#34;stable-1.</description></item><item><title>回望K8S 白话容器</title><link>https://pinkhello.cc/posts/18-%E5%9B%9E%E6%9C%9Bk8s-%E7%99%BD%E8%AF%9D%E5%AE%B9%E5%99%A8/</link><pubDate>Sat, 15 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/18-%E5%9B%9E%E6%9C%9Bk8s-%E7%99%BD%E8%AF%9D%E5%AE%B9%E5%99%A8/</guid><description>进程开启 容器, 到底是什么? 前面提出: 容器是一种沙盒技术. 就是一个集装箱, 把应用装起来的技术. 这样, 应用与应用之间有了边界不至于互相干扰; 有了这些集装箱, 也方便搬来搬去.
码农都知道可执行的二进制文件是代码的可执行镜像(executable image). 一旦程序执行起来, 内存数据、寄存器的值、堆栈的指令、打开的文件等这些集合汇集成一个程序的计算机执行环境总和: 进程.
进程: 静态表现是程序, 动态表现计算机的数据和状态的总和。
容器的核心功能, 就是通过约束和修改进程的动态表现, 从而为其创造一个&amp;quot;边界&amp;quot;.
Cgroups 技术 制造约束的主要手段 Namespace 技术 修改进程视图的主要方法 docker run , -it 告诉 Docker 启动容器后, 需要分配一个文本输入/输出环境, 也就是 TTY, 跟容器的标准输入相关联, 这样我们就可以和这个Docker容器进行交互了。而 /bin/sh 就是我们在 Docker 容器里运行的程序.
&amp;gt; docker run -it busybox /bin/sh / # 帮我启动一个容器, 在容器里执行 /bin/sh, 并且给我分配一个命令行终端跟这个容器进行交互, 在这个执行环境下可以完全执行LINUX命令,且与宿主机完全隔离在不同的世界中.
Docker对被隔离应用的进程空间做了手脚, 使得这些进程只能看到重新计算的进程编号, 可是实际上, 他们在宿主机的操作系统里, 还是原来的第N号进程. 这种技术就是Linux内部的Namespace机制。
Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如：</description></item><item><title>回望K8S 小鲸鱼容器技术</title><link>https://pinkhello.cc/posts/17-%E5%9B%9E%E6%9C%9Bk8s-%E5%B0%8F%E9%B2%B8%E9%B1%BC%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/</link><pubDate>Fri, 14 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/17-%E5%9B%9E%E6%9C%9Bk8s-%E5%B0%8F%E9%B2%B8%E9%B1%BC%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/</guid><description>什么是容器 在容器之前, 火爆云计算市场的是 PAAS, PAAS已经深入人心. 那时候突然有一家公司 dotCloud 剑走偏锋, 直接开源出了 Docker 项目，并且直接面向的社区。 这样的做法直接将当时的PAAS流主要公司打的屁滚尿流。
回头看, PAAS 最核心的是隔离环境,或者叫 沙盒,在我看来也就是 容器. 而 Docker 项目和 Cloud Foundry 的容器没有太大的不同,但是它为什么能针对 PAAS进行了一场快速的闪电战呢？
对的, 就是 Docker 镜像, 这个小小的创新, 迅速改变了云计算的发展轨迹! Docker 镜像解决的是 打包 问题。也许有人说Docker 镜像就是一个压缩包。但是就是这个压缩包包含了完整的操作系统文件和目录, 包含了整个应用所需要的依赖，一包在手, 你可以轻易的运行你的沙盒,并且本地环境与云端环境高度一致（这是最宝贵的）。
Docker给PAAS进行了致命打击, 提供了便利的打包机制, 面向后端开发者来说, 屏蔽了机器、内核等技术细节, 避免了在不同环境间的差异引入的试错成本。是一次解放生产力的革命。当然很多开发者用脚投票, 了结了PAAS时代。
Docker 三大利器 Docker项目的高调开源, 解决了打包和发布困扰运维的技术难题，同时它也第一次纯后端的概念通过友好的设计和封装交付到了开发者的手里。 Swarm,Docker是创建和启停容器的工具,那么Swarm是为了向平台化发展而提出的。它提供了完整的整体对外提供集群管理功能,它的亮点是完全使用Docker原本的管理容器的API来完成集群管理 # Swarm多机环境下，指令会被Swarm拦截处理，后面通过调度算法找到合适的Docker Daemon运行 docker run -H &amp;#34;Swarm集群API&amp;#34; &amp;#34;我的容器&amp;#34; Compose(Fig)项目, 这是第一次在开发者面前提出 容器编排(Container Orchestration)概念。 应用容器 A, 数据库容器B, 负载均衡容器C, Compose 允许 A、B、C 三个容器定义在配置文件中, 并指定关联关系.</description></item><item><title>Hexo迁移Hugo</title><link>https://pinkhello.cc/posts/16-hexo%E8%BF%81%E7%A7%BBhugo/</link><pubDate>Tue, 11 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/16-hexo%E8%BF%81%E7%A7%BBhugo/</guid><description>为什么迁移 Hugo Hugo 使用比 Hexo 简单, 只有单独的一个二进制文件 苦于 Hexo 的 NodeModule 管理 迁移成本更低, 结合 Github Action 实现 Markdown 文章发布, 自动更新至静态站 规划：加入自定义域名以及做静态资源CDN做的加速 前置工作 1、 之前基本所有的博客都托管与 github,这次也不例外, 复用 https://pinkhello.github.io,创建两个项目
pinkhello.github.io template 仓库 pinkhello.github.io.source private 仓库 2、准备OpenSSH私钥和公钥
pinkhello.github.io 仓库 添加 settings -&amp;gt; Deploy keys -&amp;gt; Add Deploy Key (将公钥添加进去、注意允许 Write) pinkhello.github.io.source 仓库 添加 settings -&amp;gt; Actions secrets -&amp;gt; New Repository Secret ( NAME : ACTION_DEPLOY_KEY, Value: 私钥 ) 3、git clone pinkhello.</description></item><item><title>Kafka与Debezium构建CDC管道</title><link>https://pinkhello.cc/posts/13-kafka%E4%B8%8Edebezium%E6%9E%84%E5%BB%BAcdc%E7%AE%A1%E9%81%93/</link><pubDate>Tue, 04 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/13-kafka%E4%B8%8Edebezium%E6%9E%84%E5%BB%BAcdc%E7%AE%A1%E9%81%93/</guid><description>建设篇 1、什么是 debezium? https://debezium.io/
Tutorial https://debezium.io/documentation/reference/1.3/tutorial.html
2、Debezium 如何工作的 2.1 Debezium 支持的数据库类型 MySQL MongoDB PostgreSQL Oracle SQL Server Db2 Cassandra 2.2 Debezium 三种方式运行 Kafka Connect Debezium Server Embedded Engine https://github.com/debezium/debezium-examples/tree/master/kinesis
3、在 K8S 中构建基础Debezium集群环境 镜像准备
kafka | debezium https://hub.docker.com/r/debezium/kafka zookeeper | debezium https://hub.docker.com/r/debezium/zookeeper connect | debezium https://hub.docker.com/r/debezium/connect schema-registry | confluentinc https://hub.docker.com/r/confluentinc/cp-schema-registry ps： debezium 参考地址 https://github.com/debezium/docker-images confluentinc 参考地址 https://github.com/confluentinc/cp-all-in-one/tree/latest/cp-all-in-one
3.1 K8S基础知识 kafka 与 zookeeper 建设为 stateful 状态集群 schema-registry 主要为了 支持 avro 格式这些不需要写到 kafka 消息头里面，减少消息的大小，额外的服务，属于 kafka 生态，存储依赖 kafka broker保证稳定性。 k8s steteful 集群 0&amp;hellip;~ n 个 POD zookeeper 里面 zoo.</description></item><item><title>几个关于kafka的知识点</title><link>https://pinkhello.cc/posts/11-%E5%87%A0%E4%B8%AA%E5%85%B3%E4%BA%8Ekafka%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/</link><pubDate>Sun, 02 May 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/11-%E5%87%A0%E4%B8%AA%E5%85%B3%E4%BA%8Ekafka%E7%9A%84%E7%9F%A5%E8%AF%86%E7%82%B9/</guid><description>认识kafka Kafka 是分布式消息系统， Apache 的子项目。标语也变了&amp;quot;分布式流平台&amp;quot;， 与传统的消息系统不同点在于
分布式的，易于扩展 为发布和订阅提供了高吞吐 支持多订阅者，在失败的时候能自动平衡消费者 消息的持久化 kafka 的架构 几点？
Kafka 的 Topic 和 Partition 内部如何存储？ 与传统的消息系统相比， Kafka 消费模型有啥优点？ Kafka 是如何实现分布式数据存储和数据的读取？ Kafka 架构 一个 Kafka 集群，多个 Producer ，多个 Consumer ，多个 Broker ， 选举 Leader 以及在 Consumer Group 发生变化时进行 reblance 。
Broker 消息中间件的处理节点，一个 Kafka 节点就是一个 Broker ， 一个或者多个 Broker 组成 Kafka 集群 Topic Kafka 根据 Topic 对 Message 进行归类，发布到 Kafka 的每条 Message 都要指定 Topic Producer 向 Broker 发生 message Consumer 从 Broker 读取 message Consumer Group 每个 Consumer 属于特定的 Group，一个 Message 可以发送给不同的 Consumer Group ，但是同一个 Group 下的只有一个 Consumer 能消费该 Message Partition 物理概念，一个 Topic 下可以分为多个 Partition, 每个 Partition 下是有序的。 下面来讲述 上面为问题啊</description></item><item><title>多域名下的SSH</title><link>https://pinkhello.cc/posts/10-%E5%A4%9A%E5%9F%9F%E5%90%8D%E4%B8%8B%E7%9A%84ssh/</link><pubDate>Fri, 02 Apr 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/10-%E5%A4%9A%E5%9F%9F%E5%90%8D%E4%B8%8B%E7%9A%84ssh/</guid><description>前言 有时候我们，有多个 git 账号（Gitlab、GitHub），这时候如果是同一个账号（邮箱注册），那不会有问题，但是如果不是相同的账号呢，我们在使用 SSH KEY 做免密登录时候，头痛了。
这个时候我们需要针对不同的账号，生成不同的 SSH Key，并且配置不同的域名使用不同的Key
生成一个 SSH KEY ssh-keygen -t rsa -C &amp;#34;username@email.com&amp;#34; 一路 Enter，并且在生成时候指定名字，（不指定名字会使用默认的）得到
id_rsa # 私钥 id_rsa.pub # 公钥 重复上一个步骤，生成多个 私钥和公钥 github_id_rsa github_id_rsa.pub gitlab_id_rsa gitlab_id_rsa.pub 配置相应的域名对应的 SSH-KEY 本地目录 ~/.ssh/ 下，查阅有没有 config 文件, 不存在就新建 config 文件 Host github HostName github.com User UserName PreferredAuthentications publickey IdentityFile ~/.ssh/github_id_rsa Host gitlab HostName gitlab.com User UserName PreferredAuthentications publickey IdentityFile ~/.ssh/gitlab_id_rsa 将密钥添加进入 SSH-AGENT 中 ssh-add ~/.</description></item><item><title>使用githook统一codestyle</title><link>https://pinkhello.cc/posts/09-%E4%BD%BF%E7%94%A8githook%E7%BB%9F%E4%B8%80codestyle/</link><pubDate>Tue, 30 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/09-%E4%BD%BF%E7%94%A8githook%E7%BB%9F%E4%B8%80codestyle/</guid><description>gradle 优化 build 指定 -g cache 缓存 checkstyle 实践 基础镜像包含 checkstyle.xml 或者 放到远程其他可被拉取到的存储介质 ，防止项目成员改动 gitlab-ci beforeScript 标签执行命令 copy /checkstyle.xml 进入项目，(覆盖项目中存在的). gradle 编译的话 将 maven-publish.gradle repos.gradle checkstyle.gradle(checkstyle 插件配置 版本以及 configFile) 抽出放到公共的地方，防止项目团队成员改的. maven 的话，可以在公共的顶级继承 pom 里面指定变量checkstyle.config.location. mvn checkstyle -Dcheckstyle.config.location=checkstyle.xml git hook 实践 每个项目里面 .git/hooks 里面有很多的 hook 模板
客户端钩子包括:pre-commit、prepare-commit-msg、commit-msg、post-commit等，主要用于控制客户端git的提交工作流。
服务端钩子：pre-receive、post-receive、update，主要在服务端接收提交对象时、推送到服务器之前调用。
今天实践的是 客户端钩子，优化减少不符合规范或者低质量代码进入 gitflow 流程.
pre-commit 和 commit-msg 是今天的主角，pre-commit 执行与 git add 之后，在进行 git commit 之前进行的操作. 可以用来进行 code check code lint 等等, commit-msg 执行与 git commit 常用于补全 git commit message check msg 等等 当然还有其他骚操作的功能，可以通知，等等，做多种自动化</description></item><item><title>Gradle多模块项目模板化</title><link>https://pinkhello.cc/posts/08-gradle%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E6%A8%A1%E6%9D%BF%E5%8C%96/</link><pubDate>Mon, 29 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/08-gradle%E5%A4%9A%E6%A8%A1%E5%9D%97%E9%A1%B9%E7%9B%AE%E6%A8%A1%E6%9D%BF%E5%8C%96/</guid><description>前言 Maven 冗余， Gradle 简单轻便 公司原有的 CI/CD 流程，借助 Maven 插件 build Docker Image,改为原生 Docker Runner 原始构建
1、多模块项目 project - app - src/main/[java|resources] | src/test/[java|resources] # classpath - Dockerfile # Dockerfile - build.gradle # APP 模块 gradle 配置 - sdk # SDK 模块 可有可无 - src/main/java - build.gradle # SDK 的 gradle 配置 - deploy # delpoy 项目 注意 checkstyle 相关配置在这里面 - checkstyle/** - **.</description></item><item><title>Fabric使用</title><link>https://pinkhello.cc/posts/07-fabric%E4%BD%BF%E7%94%A8/</link><pubDate>Sun, 28 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/07-fabric%E4%BD%BF%E7%94%A8/</guid><description>docker 加入systemctl环境并启动docker 快速安装docker
curl -sSL https://get.daocloud.io/docker | sh systemctl enable docker systemctl start docker docker-compose 安装 走外网或者 github 太慢,可以使用内部加速
curl -L &amp;#34;https://github.com/docker/compose/releases/download/X.XX.X/docker-compose-$(uname -s)-$(uname -m)&amp;#34; -o /usr/local/bin/docker-compose curl -L https://get.daocloud.io/docker/compose/releases/download/1.26.2/docker-compose-`uname -s`-`uname -m` &amp;gt; /usr/local/bin/docker-compose fabric 自动运维 python 虚拟环境安装 参考
创建一个独立的虚拟环境
cd 目标目录 virtualenv --no-site-packages venv 激活虚拟环境
source venv/bin/activate python pip 安装 Fabric pip install fabric3 python pip 导出依赖 pip freeze &amp;gt; requirements.txt 其他python pip 导入安装 pip install -r requirements.txt Fabric 文档 fabfiles 文档 # encoding=utf-8 from fabric.</description></item><item><title>高性能队列Disruptor</title><link>https://pinkhello.cc/posts/06-%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97disruptor/</link><pubDate>Mon, 15 Mar 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/06-%E9%AB%98%E6%80%A7%E8%83%BD%E9%98%9F%E5%88%97disruptor/</guid><description>背景 Disruptor 是 外汇交易公司LMAX开发的高性能队列、研发是为了解决内存队列延迟问题。 Disruptor 一般用于线程间的消息传递。 Disruptor GitHub 地址
Disruptor 介绍 理解 Disruptor 最好的方式，选择一个最接近熟悉的样本进行比较。在这个前提下，可以选择 Java 中的 BlockingQueue. 和队列相似，Disruptor 也是在同一个进程中不同的线程之间进行传递数据的（例如消息或者事件），同时 Disruptor 提供了一些将关键功能和队列分开的特性：
向消费者发送多播事件 消息者依赖关系图 预先为事件分配内存 可选的（无锁） Disruptor 核心概念 在我们理解Disruptor如何工作之前，了解下核心概念
Ring Buffer 环形数组设计，为了避免垃圾回收，采用的数组结构，从3.0开始，环形缓冲区主要存储和更新在Disruptor中移动的数据（事件） Sequence Disruptor 每个消费者(EventProcessor)维护一个 Sequence，并发的大多数代码都依赖 Sequence 值的改动，所以 Sequence 支持 AtomicLong 的大部分也行, 唯一不同的是 Sequence 包含额外的功能来阻止Sequence和其他值之间的伪共享(false sharing) Sequencer
Disruptor 核心逻辑, 两个实现: 单生产者和多生产者。他们实现了生产者与消费者之间的快速传递的并发算法。 Sequence Barrier 由 Sequencer 生成，包含此 Sequencer 发布的 Sequence 指针以及依赖的其他消费者的 Sequence。包含了消费者检查是否有可用的事件的代码。 Wait Strategy 消费者等待事件的策略，这个事件由生产者放入，决定了消费者怎么等待生产者将事件放入 Disruptor Event 生产者与消费者传递的事件，完全由用户定义 EventProcessor 处理事件的主要循环（main event loop），包含了一个 Sequeuece.</description></item><item><title>OAuth2.0 那点事</title><link>https://pinkhello.cc/posts/05-oauth2.0%E9%82%A3%E7%82%B9%E4%BA%8B/</link><pubDate>Wed, 10 Feb 2021 08:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/05-oauth2.0%E9%82%A3%E7%82%B9%E4%BA%8B/</guid><description>OAuth2.0 是什么? OAuth2.0 Framework RFC 6749 [https://tools.ietf.org/html/rfc6749]
OAuth 就是一种授权机制，它介于客户端与资源所有者的授权层，为了分离不同的角色。 在资源所有者同意并向客户端颁发令牌后，客户端携带令牌可以访问部分或全部资源。
OAuth2.0是OAuth协议的一个版本，为2.0版本。有意思的是 2.0与 1.0并不兼容。
OAuth2.0 授权方式 获取授权的过程
授权码(authorization-code) 隐藏式(implicit) 密码(password) 客户端凭证(client credentials) 不管哪种方式，都需要在第三方应用申请令牌之前，需要在系统中申请身份唯一标识: 客户端ID Client ID和 客户端秘钥 Client Secret. 这样能确保Token不被恶意使用。
授权重要的参数和指标:
response_type响应类型: code(要求返回授权码),token(要求返回授权Token) client_id客户端身份标识 client_secret客户端秘钥 redirect_uri重定向地址 scope授权范围, read只读权限, all全部权限 grant_type授权方式 authorization_code(授权码)、password(密码)、client_credentials(凭证)、refresh_token(更新令牌) state应用程序传递的一个随机数，防止 CSRF攻击 授权码 在访问第三方应用先申请一个授权码，然后再用授权码获取令牌.这种方式也是最常用的流程，安全性也是最高的，适用于有后端的Web应用。授权码通过前端传送，令牌存储在后端。所有的和资源服务器的交互都在服务端完成，避免了令牌的泄露。 授权码和令牌的在 浏览器和客户端WEB应用以及资源服务器的交互流程大致如下: 1.2.3.4 用户选择 Google登陆 yelp.com 3.4 Yelp.com请求用户授权 Google权限 5.6 用户同意后返回授权码 7.8 Yelp.com通过授权码 会向 Google发起请求Token 9 验证必要参数，返回 Token 10.11 操作请求 隐藏式 密码式 顾名思议,在自己的系统输入第三方系统的账号密码,自己的系统拿账号密码去申请令牌，响应题里面返回token</description></item><item><title>如何构建一个简单的RPC调用</title><link>https://pinkhello.cc/posts/04-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84rpc%E8%B0%83%E7%94%A8/</link><pubDate>Wed, 13 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/04-%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84rpc%E8%B0%83%E7%94%A8/</guid><description>1、什么叫RPC?
RPC构成
RPC Consumer RPC Provider ConfigServer 1、Provider 启动 ConfigServer 注册服务 2、Consumer 启动 ConfigServer 订阅服务， 3、发起调用 Consumer &amp;mdash;&amp;gt; Provider 4、响应调用 Consumer &amp;lt;&amp;mdash; Provider 2、什么是 Netty ? https://netty.io/
3、现有的开源的项目是否使用了 Netty ?
Dubbo Grpc Spark &amp;hellip;. 4、RPC Provider 启动
Netty Server 方式启动 Rpc 服务的注册 5、RPC Consumer 启动
Netty Client 方式启动 RPC 泛化调用、通过字节码基于反射来实现远程调度 Consumer 服务订阅 启动时建立长连接 6、从第四可以看出，多个 Provider 是由一个 NettyServer 提供的，通过 HandlerMap 映射找到对应的 Ioc Bean，完成服务调用</description></item><item><title>String为什么设计成final</title><link>https://pinkhello.cc/posts/03-string%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E8%AE%A1%E6%88%90final/</link><pubDate>Tue, 12 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/03-string%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%BE%E8%AE%A1%E6%88%90final/</guid><description>String源码剖析 public final class String implements java.io.Serializable, Comparable&amp;lt;String&amp;gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; /** * Class String is special cased within the Serialization Stream Protocol.</description></item><item><title>关于final的思考</title><link>https://pinkhello.cc/posts/02-%E5%85%B3%E4%BA%8Efinal%E7%9A%84%E6%80%9D%E8%80%83/</link><pubDate>Mon, 11 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/02-%E5%85%B3%E4%BA%8Efinal%E7%9A%84%E6%80%9D%E8%80%83/</guid><description>关于final的思考 final 是声明数据域最终的,不可以修改的，常见的 是类的 序列化ID String 类，其数据域都是 final 的 修改 final 修饰的属性 反射修改 final 修饰的数据域【非常成功的修改了】
public class Test { private final String name = &amp;#34;hello world&amp;#34;; public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { Test test = new Test(); Field field = test.getClass().getDeclaredField(&amp;#34;name&amp;#34;); field.setAccessible(true); field.set(test,&amp;#34;HELLO, WORLD!&amp;#34;); System.out.println(field.get(test)); System.out.println(test.name); } } 输出 Hello, WORLD! hello world 第一个输出是因为说明运行成功，修改final修饰的对象的属性成功修改；
但是第二个输出，表明了我直接使用 name 的属性却还是输出端额原来的值.</description></item><item><title>一致性哈希算法</title><link>https://pinkhello.cc/posts/01-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/</link><pubDate>Sun, 10 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/01-%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/</guid><description>分布式均衡寻址算法 在分布式集群中，对机器的添加删除，或者机器故障后自动脱落集群的操作是分布式集群管理的基本功能。
在集群环境中，判断分布式寻址算法好坏的原则：
平衡性（Balance） 单调性（Monotonicity） 分散性（Spread） 负载（Load） Hash(Object)%N 集群N台机器，根据N取模，路由到对应的机器，但是缺点在于，对于机器的添加删除，已经缓存的数据都失效、严重违反单调性， 大量的缓存重建
假设0-3个节点、20个数据: 进行取模后分布: 扩容后: 当前只有4个数据能命中。命中率 4/20 = 20% ,命中率底下，并且有大量缓存需要重建
一致性Hash ( DHT ) 公共哈希函数和哈希环 Hash算法设计: 采取取模方式，按常用的 Hash 算法将对应的 Key 哈希到一个具有 2^32 次方的桶空间中，即 0 ~ (2^32)-1 的数字空间。想想一下，将数字首位相连，组成一个闭合的环形。 对象(Object)映射到哈希环 把对象映射到 0-2^32-1 空间里，假设有4个对象 object1-4 ，映射进hash环状 缓存(Cache)映射到哈希环 下面将 Cache 映射进 Hash 空间，假设现在有三个cache：
基本思想就是 Object 和 Cache 都映射到同一 Hash 数值空间中，并且使用相同的 Hash算法，可以使用 Cache 的 IP地址或者其他因子）</description></item><item><title>Threadlocal 魔法</title><link>https://pinkhello.cc/posts/00-threadlocal-%E9%AD%94%E6%B3%95/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-threadlocal-%E9%AD%94%E6%B3%95/</guid><description>ThreadLocal 详解 前言 对于 ThreadLocal 的使用，并不难，这次主要讲述 ThreadLocal 的实现方式以及原理
ThreadLocal 是什么 ThreadLocal 为解决多线程并发问题提供的一种新的思路。
当使用 ThreadLocal 维护变量的时候，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每个线程都可以独立修改自己的副本，而不会修改到其他人的变量副本。
从线程角度看，Local 即本地意思，目标变量就像是线程的本地变量。
原理 ThreadLocal 是连接 Thread 与 ThreadLocalMap 粘合剂，是用来处理 Thread 的 ThreadLocalMap 属性， 包括 initialValue() 变量，set 对应的变量，get 对应的变量。
ThreadLocalMap 用来存储数据，采用类似HashMap的机制，存储了以ThreadLocal为Key，目标数据为Value的Entry键值对数组结构。
Thread 有个 ThreadLocalMap 的属性，存储的数据存放在此处。
Thread、ThreadLocal、 ThreadLocalMap的关系 ThreadLocalMap 是 ThreadLocal 的内部类，有 ThreadLocal创建，Thread有 ThreadLocal.ThreadLocalMap 类型的属性，源码如下
Thread public class Thread implements Runnable { /* * ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class.</description></item><item><title>数据结构与算法 01 优先队列</title><link>https://pinkhello.cc/posts/00-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-01-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-01-%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/</guid><description>为什么需要优先队列 队列是一种先进先出的数据结构，所有元素优先级一样，完全遵守先进先出的规则。但是往往现实情况下，这种公平需要被打破。它是一个动态变化的过程，可能有一些需要优先，一些需要降低优先级。且这些数据是一个动态变化的过程，所以需要维系这个优先级队列。
优先队列的实现方式 数组实现 链表</description></item><item><title>算法 Bitmap</title><link>https://pinkhello.cc/posts/00-%E7%AE%97%E6%B3%95-bitmap/</link><pubDate>Sat, 09 Feb 2019 10:00:00 +0800</pubDate><guid>https://pinkhello.cc/posts/00-%E7%AE%97%E6%B3%95-bitmap/</guid><description>bitmap 原理 bitmap字面为位图映射, 原理是使用一个 bit 标记某个元素对应的 value，而 key 即该元素。因为只有一个 bit 来存储一个数据, 因而可以大大的节省空间。
数值映射: 假如对 0-31 个内的3个元素（10, 17, 28）进行排序,可以采用 BitMap 方法, 如下图, 对应的包含的位置将对应的值从 0 变更为 1 假如需要进行排序和检索，只需要依次遍历这个数据结构，碰到 1 的情况，数据存在
字符串映射: 字符串也可映射，只不过需要经过一个Hash步骤,通过映射关系可以判断字符串是否存在。但是因为 Hash是将不确定长度的值变更为确定大小的值,存在Hash冲突性，所以一般要最大化的判断一个字符串是否真的存在，可以将这个字符串经过不同的Hash函数映射不同的位置。
bitmap 的 建立、查找、添加、删除、判断 原理 建立 Bitmap 的创建可以使用 byte 数组， 1 byte = 8 bit (也可使用 int 数组, 1 int = 32 bit, long 数组, 1 long = 64 bit) 也就是说到最后的数据的大小建立只需要创建 数组长度为 int[ 1 + N/32 ] byte[ 1 + N/8 ] long[ 1 + N/64 ] 即可存储，N表示要存储的最大的值。</description></item></channel></rss>